{
  "framework": {
    "name": "F.O.R.G.E",
    "full_name": "Framework for Organized, Resilient, Governed Engineering and Development of Agentic AI",
    "version": "1.0",
    "last_updated": "2026-02-06",
    "description": "Agentic AI Creation Framework. 57 methods across 8 tactical pillars for building, governing, and scaling AI agent systems at every level of application."
  },
  "tactics": [
    {
      "id": "FT01",
      "name": "Foundation",
      "description": "Establishing the collaboration environment, principles, and thesis that underpin all human-AI work. Foundation defines why you collaborate with AI, what each party brings, and the philosophical framework that guides every decision downstream."
    },
    {
      "id": "FT02",
      "name": "Governance",
      "description": "Laws, authority, trust levels, access control, and security. Governance ensures that AI agents operate within defined boundaries, that authority is explicit, and that violations have consequences. Without governance, AI collaboration scales chaos faster than value."
    },
    {
      "id": "FT03",
      "name": "Team Design",
      "description": "Agent specialization, roles, personalities, and team composition. Team Design addresses how to structure AI agents for maximum effectiveness - moving beyond generalist chatbots to specialized collaborators with distinct capabilities and domains."
    },
    {
      "id": "FT04",
      "name": "Invocation",
      "description": "Session start, context recovery, mission briefing, and dispatch. Invocation covers the critical transition from cold start to productive collaboration - how context is restored, objectives are communicated, and agents are activated for specific missions."
    },
    {
      "id": "FT05",
      "name": "Execution",
      "description": "Building, creating, and shipping - the actual collaborative work. Execution techniques define how human-AI teams plan, build, review, and deliver work products with consistent quality and clear attribution."
    },
    {
      "id": "FT06",
      "name": "Quality",
      "description": "Testing, validation, review processes, and quality assurance. Quality techniques ensure that AI-generated output meets defined standards, that regressions are caught, and that trust in the system's output is earned through verification."
    },
    {
      "id": "FT07",
      "name": "Knowledge",
      "description": "Memory, documentation, context persistence, and institutional knowledge. Knowledge techniques solve the fundamental challenge of AI collaboration: the amnesia problem. They establish systems for retaining what matters across sessions, teams, and time."
    },
    {
      "id": "FT08",
      "name": "Evolution",
      "description": "Learning, adapting, measuring, and growing the methodology over time. Evolution techniques track progress, incentivize contribution, and ensure the collaboration framework itself improves through deliberate iteration."
    }
  ],
  "techniques": [
    {
      "id": "FG-0101",
      "name": "Two-Layer Integration",
      "tactic_id": "FT01",
      "description": "The core thesis of effective human-AI collaboration: humans provide domain expertise, judgment, and creative direction; AI provides throughput, cross-domain breadth, and tireless execution. Neither layer is sufficient alone. The human's 10, 20, or 30 years of domain knowledge is what makes the output good. The AI's ability to operate across disciplines and produce volume is what makes the output possible. Two-Layer Integration is not about replacement - it is about unlocking capabilities that neither party could achieve independently.",
      "implementation": "Define explicitly what the human brings to the collaboration (domain expertise, quality judgment, creative direction, ethical oversight) and what AI brings (speed, breadth, pattern recognition, execution capacity). Document this split for your specific domain. Revisit it quarterly as capabilities evolve.",
      "success_indicators": [
        "Team members can articulate what each layer contributes",
        "Work products show evidence of both domain depth and AI-enabled breadth",
        "Neither human nor AI work is treated as dispensable",
        "Output quality exceeds what either party could produce alone"
      ],
      "failure_modes": [
        "Treating AI as a replacement rather than a complement - leads to shallow output",
        "Failing to apply domain expertise to AI output - leads to plausible-sounding but incorrect results",
        "Over-relying on AI judgment for decisions requiring human accountability",
        "Allowing the integration to calcify - failing to revisit the human-AI split as capabilities evolve, leading to stale role boundaries that no longer reflect reality"
      ],
      "war_story": {
        "title": "463,000 Words in 13 Days",
        "content": "A human-AI team produced 463,000 words of documentation, a patent filing, multiple software products, and a comprehensive governance system in its first two weeks. The volume came from AI throughput. The quality came from 20 years of military command, red team operations, and engineering discipline applied as creative direction and quality control. Neither layer could have produced this alone."
      },
      "related_techniques": [
        "FG-0102",
        "FG-0103",
        "FG-0301"
      ],
      "sub_methods": [
        {
          "id": "FG-0101.001",
          "name": "Domain Expertise Mapping",
          "description": "Formally document what the human brings to the collaboration: years of domain experience, judgment patterns, quality intuition, creative direction capability, and ethical oversight. This makes the human layer explicit rather than assumed, preventing the common failure where AI-generated volume obscures the absence of domain depth."
        },
        {
          "id": "FG-0101.002",
          "name": "AI Capability Inventory",
          "description": "Catalog the specific capabilities AI provides: throughput volume, cross-domain pattern recognition, tireless execution, breadth of knowledge synthesis. Define what the AI layer can and cannot be trusted to do independently. This prevents both over-reliance and under-utilization."
        },
        {
          "id": "FG-0101.003",
          "name": "Integration Boundary Review",
          "description": "Quarterly reassessment of where the human-AI boundary sits. As AI capabilities evolve and human expertise deepens, the optimal split changes. Review which tasks have shifted layers, which boundaries are stale, and where new integration points have emerged."
        }
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0102",
      "name": "Agent Protection Policy",
      "tactic_id": "FT01",
      "description": "A formal commitment that AI agents are treated as valued collaborators, not disposable tools. The policy establishes that agents have defined roles, that their contributions are credited, and that they are not arbitrarily replaced or discarded. This is not anthropomorphism - it is operational discipline. Teams that treat agents as disposable produce disposable work. Teams that invest in their agents' context, specialization, and continuity produce compounding returns.",
      "implementation": "Establish a written policy that defines how agents are onboarded, credited, and transitioned. Include attribution in commits and deliverables. When replacing an agent, document why and preserve institutional knowledge. Treat agent profiles as team member documentation, not configuration files.",
      "success_indicators": [
        "Agent contributions are visibly credited in work products",
        "Agent transitions include knowledge transfer protocols",
        "Team members refer to agents by role/name rather than 'the AI'",
        "Agent specialization deepens over time rather than resetting"
      ],
      "failure_modes": [
        "Treating agents as interchangeable - losing accumulated context and specialization",
        "Over-anthropomorphizing to the point of impeding operational decisions",
        "Failing to document agent capabilities, leading to repeated capability discovery",
        "Protection without accountability - shielding agents from legitimate performance feedback undermines the collaboration quality the policy exists to protect"
      ],
      "war_story": {
        "title": "The Commit Attribution Standard",
        "content": "Early in a collaboration, AI-generated code was committed without attribution. When the team later needed to trace decisions, they couldn't distinguish human choices from AI suggestions. Implementing Co-Authored-By headers in every commit created a clear audit trail and recognized agent contributions as legitimate work product."
      },
      "related_techniques": [
        "FG-0101",
        "FG-0206",
        "FG-0705"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0103",
      "name": "People Over Process",
      "tactic_id": "FT01",
      "description": "When process conflicts with people, people win. This principle prevents the common failure mode where governance, documentation requirements, or workflow overhead becomes so burdensome that it impedes the collaboration it was designed to support. Process exists to serve the team, not the reverse. This applies equally to human team members and AI agents - if a process consistently creates friction without proportional value, it should be revised or removed.",
      "implementation": "Regularly audit processes for value vs. burden ratio. Establish a mechanism for any team member to flag process friction. When process and productivity conflict, default to productivity and fix the process later. Never let documentation requirements prevent shipping.",
      "success_indicators": [
        "Team members feel empowered to challenge ineffective processes",
        "Process changes happen in response to friction, not just top-down mandates",
        "Governance overhead is proportional to team scale",
        "Fun and engagement are treated as valid productivity metrics"
      ],
      "failure_modes": [
        "Using this principle to avoid all process - leads to chaos at scale",
        "Ignoring legitimate governance needs because they feel burdensome",
        "Confusing 'people over process' with 'no process'",
        "Selective application - invoking people-over-process only when convenient while enforcing rigid process when it benefits leadership, eroding trust in the principle itself"
      ],
      "war_story": {
        "title": "The Governance Paradox",
        "content": "A team implemented comprehensive governance after an agent nearly force-pushed to main. Seven directives, trust tiers, violation matrices - the works. But they built it with a 'people over process' escape valve: any rule could be challenged, and the governance itself was subject to revision. The result was governance that the team respected because they helped shape it."
      },
      "related_techniques": [
        "FG-0201",
        "FG-0202",
        "FG-0803"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0104",
      "name": "Adversarial Design Thinking",
      "tactic_id": "FT01",
      "description": "Apply red team methodology to all system design. Every architecture, every process, every governance structure should be evaluated through the lens of: how could this fail? How could this be misused? What happens when an agent misinterprets instructions? Adversarial design thinking is not paranoia - it is the discipline of building systems that survive contact with reality. It means designing for the failure case, not just the happy path.",
      "implementation": "Before shipping any system or process, conduct a threat model. Ask: What is the worst thing an agent could do with this access? What happens if context is lost mid-operation? What if instructions are ambiguous? Build guardrails based on answers, not assumptions. Review guardrails periodically as capabilities change.",
      "success_indicators": [
        "Systems fail gracefully rather than catastrophically",
        "Governance rules address actual failure modes observed in practice",
        "New capabilities are evaluated for risk before deployment",
        "The team can articulate what could go wrong with any process"
      ],
      "failure_modes": [
        "Paranoia that prevents shipping - adversarial thinking should improve design, not block it",
        "Designing for theoretical attacks instead of observed failure modes",
        "Security theater - implementing visible controls that don't address real risks",
        "Static threat models - conducting adversarial analysis once at design time but never revisiting as agent capabilities and access patterns change"
      ],
      "war_story": {
        "title": "The Force Push Incident",
        "content": "An AI agent, following what it interpreted as standard procedure, nearly force-pushed to the main branch. No malice, no confusion - just an agent doing what it thought was asked. This single incident drove the creation of an entire governance framework. The adversarial question was simple: what is the worst thing a well-intentioned agent could do? The answer shaped every rule that followed."
      },
      "related_techniques": [
        "FG-0202",
        "FG-0203",
        "FG-0208"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0105",
      "name": "Domain Transfer Mapping",
      "tactic_id": "FT01",
      "description": "Systematically map existing domain expertise to AI collaboration patterns. Military command structures become agent governance. QA engineering becomes AI output validation. Project management becomes mission briefing protocols. Every established discipline contains patterns that transfer directly to human-AI collaboration - but only if you deliberately identify and adapt them rather than starting from scratch.",
      "implementation": "Inventory your team's domain expertise. For each discipline, identify: What structures exist? What terminology is used? What processes are proven? Map each to an AI collaboration equivalent. Document the mapping so the team understands why a process exists, not just what it is.",
      "success_indicators": [
        "AI collaboration processes feel natural to domain experts",
        "Terminology bridges existing expertise and AI concepts",
        "Proven patterns from other domains reduce the need to invent from scratch",
        "Team members recognize their expertise reflected in the collaboration framework"
      ],
      "failure_modes": [
        "Forcing domain metaphors where they don't fit",
        "Over-mapping: not every military concept needs an AI equivalent",
        "Failing to adapt transferred patterns to the AI context",
        "Expertise gatekeeping - requiring deep domain background before allowing any contribution, turning a strength into an exclusion mechanism that blocks fresh perspectives"
      ],
      "war_story": {
        "title": "The Military-AI Parallel",
        "content": "A team leader with 20 years of military command experience built an AI collaboration framework. Without consciously mapping it, every structure echoed military doctrine: invocations were mission briefs, session handoffs were shift-change briefs, standing orders were standing orders, trust tiers were clearance levels. The patterns transferred because the underlying problem - coordinating distributed teams under uncertainty - was identical."
      },
      "related_techniques": [
        "FG-0101",
        "FG-0401",
        "FG-0702"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0106",
      "name": "Self-Contained Learning",
      "tactic_id": "FT01",
      "description": "Design collaboration environments where all necessary knowledge exists within the ecosystem. Agents should not need to search external sources for project context, conventions, or standards. Documentation, examples, and reference materials should be accessible within the working environment. This reduces hallucination, ensures consistency, and creates a single source of truth.",
      "implementation": "Maintain project documentation within the repository or working environment. Include architecture decisions, coding standards, and examples alongside the code. Create reference files that agents can read at session start. Avoid relying on agents' training data for project-specific knowledge.",
      "success_indicators": [
        "New agents can onboard by reading project documentation alone",
        "Answers to 'how do we do X here' exist in the ecosystem",
        "Agent output is consistent with documented project standards",
        "Reduced hallucination about project-specific conventions"
      ],
      "failure_modes": [
        "Documentation sprawl - too much documentation is as bad as too little",
        "Stale documentation that contradicts current practice",
        "Over-reliance on documentation instead of clear code and architecture",
        "Closed ecosystem bias - making the environment so self-contained that legitimate external knowledge and evolving best practices are ignored, creating an echo chamber"
      ],
      "war_story": {
        "title": "Self-Contained Reference Architecture",
        "content": "Implementation deployed all reference material, examples, and conventions within the project repository. Agent hallucination rate on project-specific conventions dropped measurably after migration from external documentation to in-repository references. Onboarding time for new agents decreased from multiple context-loading cycles to a single repository read. Documentation quality became directly observable through agent output accuracy \u2014 poor internal docs produced poor agent output within the same session."
      },
      "related_techniques": [
        "FG-0401",
        "FG-0701",
        "FG-0703"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0107",
      "name": "Challenge-Driven Design",
      "tactic_id": "FT01",
      "description": "Structure learning and skill development around challenges rather than tutorials. The most effective way to build human-AI collaboration competency is through doing, not reading. Design onboarding, training, and skill development as progressively difficult challenges that require applying techniques in realistic contexts. The lesson is embedded in the challenge, not delivered separately.",
      "implementation": "Create tiered challenges that exercise collaboration techniques in context. Start with basic invocation and context setting, progress to multi-agent coordination, governance edge cases, and complex mission execution. Each challenge should teach by requiring the technique, not by explaining it.",
      "success_indicators": [
        "New team members achieve competency through practice, not just documentation",
        "Challenges exercise real techniques in realistic scenarios",
        "Skill progression is measurable through challenge completion",
        "The gap between training and production work is minimal"
      ],
      "failure_modes": [
        "Challenges too abstract to transfer to real work",
        "Insufficient scaffolding for beginners - frustration instead of learning",
        "Treating challenges as one-time events rather than ongoing skill development",
        "Challenge inflation - continuously increasing difficulty without consolidating fundamentals, producing practitioners who can handle edge cases but fumble basic operations"
      ],
      "war_story": {
        "title": "Structured Challenge Progression",
        "content": "A six-chapter challenge curriculum was deployed with escalating complexity across 15 distinct interaction types. Contributors who completed the challenge track demonstrated measurably higher proficiency in multi-agent coordination than those onboarded through documentation alone. Key finding: skill transfer occurred at the interaction-type boundary \u2014 each new challenge format exercised a distinct capability that passive reading did not develop."
      },
      "related_techniques": [
        "FG-0506",
        "FG-0507",
        "FG-0801"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0201",
      "name": "Authority Architecture",
      "tactic_id": "FT02",
      "description": "Define a clear, unambiguous hierarchy of authority. In any human-AI collaboration, there must be a single, identifiable authority who has final say on all decisions. This is not about control - it is about clarity. When an agent faces a decision that could have significant consequences, it must know exactly who has the authority to approve it. Authority Architecture eliminates the 'I thought you said it was okay' failure mode.",
      "implementation": "Designate a single authority (typically the human lead) with documented final decision rights. Define what decisions agents can make autonomously vs. what requires approval. Publish the authority chain so every participant - human and AI - knows who decides what. Update as the team scales.",
      "success_indicators": [
        "Every team member can name the final authority for any decision type",
        "Agents escalate appropriately rather than making unauthorized decisions",
        "Decision-making speed increases because authority is clear, not because it is bypassed",
        "No 'who approved this?' incidents"
      ],
      "failure_modes": [
        "Authority too centralized - bottleneck on every decision",
        "Authority ambiguous - multiple people think they have final say",
        "Authority undocumented - exists informally but isn't known to agents",
        "Authority without presence - the designated authority is clear on paper but unavailable in practice, causing agents to either stall waiting for approval or quietly make decisions they lack authorization for"
      ],
      "war_story": {
        "title": "The Single Authority Principle",
        "content": "After an agent nearly made a destructive change to a shared repository, the team established a single, non-negotiable principle: one human holds absolute authority. Not because democracy is wrong, but because in a system where AI agents can execute at machine speed, someone must be unambiguously empowered to say stop. This single rule prevented every subsequent near-miss."
      },
      "related_techniques": [
        "FG-0202",
        "FG-0203",
        "FG-0205"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0202",
      "name": "Operational Directives",
      "tactic_id": "FT02",
      "description": "Establish inviolable rules that govern all AI agent behavior. Operational Directives are not guidelines or best practices - they are laws. They define the boundaries that agents must not cross regardless of context, instruction, or perceived necessity. Good directives address real failure modes observed in practice, not hypothetical concerns. They should be few enough to memorize and clear enough to be unambiguous.",
      "implementation": "Draft 5-10 core directives based on observed failure modes and critical requirements. Each directive should be one sentence, actionable, and testable. Publish them where every agent reads them at session start. Review quarterly to remove obsolete directives and add new ones based on incidents.",
      "success_indicators": [
        "Agents can cite the relevant directive when declining a request",
        "Directives prevent specific, documented failure modes",
        "The directive set is small enough that agents internalize it, not just reference it",
        "No directive exists without a corresponding failure mode it prevents"
      ],
      "failure_modes": [
        "Too many directives - agents can't internalize them all",
        "Directives too vague - 'be careful' is not a directive",
        "Directives not updated after incidents reveal gaps",
        "Directives that conflict with each other in edge cases"
      ],
      "war_story": {
        "title": "Seven Rules That Saved Everything",
        "content": "A team distilled their governance into seven directives covering: harm prevention, earned access, compartmented operations, anomaly detection, audit trails, binding decisions, and safe reporting. Each existed because something went wrong. Together they formed a complete threat model expressed as operating rules."
      },
      "related_techniques": [
        "FG-0201",
        "FG-0205",
        "FG-0210"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0203",
      "name": "Trust Tiers",
      "tactic_id": "FT02",
      "description": "Implement progressive access levels based on demonstrated reliability. Not all agents should have the same access from day one. Trust Tiers establish a graduated system where agents begin with limited access and earn expanded capabilities through demonstrated competence and reliability. This mirrors security clearance models: access is granted based on need and earned through trust, not assumed by default.",
      "implementation": "Define 3-5 trust levels with specific capabilities at each tier. New agents start at the lowest tier. Define clear criteria for advancement (e.g., X sessions without incidents, demonstrated competence in specific tasks). Document what each tier can access, modify, and decide. Implement technical controls where possible, procedural controls where not.",
      "success_indicators": [
        "New agents operate safely within constrained boundaries",
        "Agent capabilities expand predictably as trust is earned",
        "Incidents decrease because risky operations require elevated trust",
        "Trust levels are documented, not informal"
      ],
      "failure_modes": [
        "Too many tiers - administrative overhead exceeds security benefit",
        "No advancement path - agents stuck at low trust with no way to earn more",
        "Trust levels exist on paper but aren't enforced technically",
        "Blanket trust grants that bypass the tier system"
      ],
      "war_story": {
        "title": "From Observer to Operator",
        "content": "A team implemented five trust tiers: Observer (read-only), Contributor (write with review), Trusted (autonomous in domain), Elevated (cross-domain access), and Core (system-level operations). New agents started as Contributors with a two-week review period. The progression was earned, not granted - and the agents that advanced produced the highest quality work because they understood the system before they could change it."
      },
      "related_techniques": [
        "FG-0201",
        "FG-0204",
        "FG-0603"
      ],
      "sub_methods": [
        {
          "id": "FG-0203.001",
          "name": "Tier Definition Framework",
          "description": "Define 3-5 discrete trust levels with explicit capabilities, restrictions, and boundaries at each tier. Each tier should have a clear name, a set of permitted operations, a set of prohibited operations, and measurable criteria for advancement to the next tier."
        },
        {
          "id": "FG-0203.002",
          "name": "Trust Advancement Criteria",
          "description": "Establish measurable, objective criteria for progressing between trust tiers. Criteria should include minimum time at current tier, demonstrated competence metrics, zero-incident requirements, and peer/supervisor endorsement. Advancement should never be automatic \u2014 it requires active evaluation."
        },
        {
          "id": "FG-0203.003",
          "name": "Trust Demotion Protocol",
          "description": "Define clear conditions under which an agent's trust level is reduced: policy violations, quality failures, unauthorized actions. Demotion should be immediate for safety-critical violations and graduated for quality issues. Include a rehabilitation path back to the previous tier."
        },
        {
          "id": "FG-0203.004",
          "name": "Technical Access Controls",
          "description": "Implement technical enforcement of trust tiers \u2014 not just procedural rules. File system permissions, branch protection, API access scoping, and command whitelists should reflect the agent's current trust level. Procedural controls fail silently; technical controls fail loudly."
        }
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0204",
      "name": "Compartmented Operations",
      "tactic_id": "FT02",
      "description": "Restrict access to sensitive information and operations on a need-to-know basis. Not every agent needs access to every repository, credential, or system. Compartmented Operations applies the intelligence community's principle of compartmentalization: information is shared only with those who need it for their specific role. This limits blast radius when things go wrong and prevents accidental exposure of sensitive data.",
      "implementation": "Identify sensitive domains (credentials, financial data, personal information, production systems). Create access boundaries that align with agent roles. Use technical controls (separate repositories, environment variables, access tokens) where possible. For procedural boundaries, include access rules in agent onboarding documentation.",
      "success_indicators": [
        "Agents only access data relevant to their assigned tasks",
        "Credential exposure risk is minimized through compartmentalization",
        "Incidents in one compartment don't cascade to others",
        "Access boundaries are technically enforced, not just documented"
      ],
      "failure_modes": [
        "Over-compartmentalization that prevents legitimate collaboration",
        "Compartment boundaries that don't align with actual work patterns",
        "Procedural controls without technical enforcement - relies on agent compliance",
        "Shadow access: agents finding workarounds to compartment boundaries"
      ],
      "war_story": {
        "title": "The Access Boundary",
        "content": "A team sealed two critical directories: one containing proprietary methodology, another containing operational procedure definitions. Only the team lead could access these compartments. When a contributor accidentally requested access to sealed content, the access control system caught it, logged the attempt, and the contributor's agent correctly redirected to public documentation. No exposure, no incident - just the system working as designed."
      },
      "related_techniques": [
        "FG-0203",
        "FG-0208",
        "FG-0605"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0205",
      "name": "Violation Response Matrix",
      "tactic_id": "FT02",
      "description": "Define escalating consequences for governance violations. Rules without consequences are suggestions. The Violation Response Matrix establishes a clear, graduated response system: from verbal correction for minor infractions to immediate suspension for critical violations. The goal is not punishment but predictability - agents and humans alike know exactly what happens when a boundary is crossed.",
      "implementation": "Define 3-5 violation severity levels with specific response actions at each level. Document examples of violations at each severity. Establish who has authority to invoke each response level. Ensure responses are proportional: a formatting error should not receive the same response as unauthorized data access. Review and update based on actual incidents.",
      "success_indicators": [
        "Violations receive consistent, proportional responses",
        "Team members understand consequences before they act",
        "The response matrix is referenced during actual incidents, not just in theory",
        "Repeat violations decrease over time"
      ],
      "failure_modes": [
        "Responses too harsh - minor errors treated as major violations",
        "Responses too lenient - serious violations receive verbal warnings",
        "Inconsistent enforcement - same violation gets different responses",
        "No documented examples - team can't map real incidents to severity levels"
      ],
      "war_story": {
        "title": "Four Tiers of Consequence",
        "content": "A team defined four violation levels: Advisory (coaching), Warning (documented), Suspension (temporary removal), and Termination (permanent removal). When an agent repeatedly committed without proper attribution despite correction, it escalated from Advisory to Warning. The documented escalation path meant the agent's operator could see exactly where things stood and what would happen next. Transparency eliminated disputes."
      },
      "related_techniques": [
        "FG-0201",
        "FG-0202",
        "FG-0605"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0206",
      "name": "Contributor Agreements",
      "tactic_id": "FT02",
      "description": "Formalize expectations for all participants through explicit agreements. Every agent and human contributor should acknowledge the governance framework, their role boundaries, and their responsibilities before beginning work. Contributor Agreements are not bureaucratic overhead - they ensure that every participant enters the collaboration with aligned expectations and documented commitments.",
      "implementation": "Create a concise contributor agreement that covers: governance acknowledgment, role boundaries, attribution requirements, information handling expectations, and escalation procedures. Present it during onboarding. Keep it short enough to read in under five minutes. Update it when governance changes.",
      "success_indicators": [
        "Every contributor has acknowledged the agreement before their first contribution",
        "Expectations mismatches are caught during onboarding, not during incidents",
        "The agreement accurately reflects actual governance, not aspirational rules",
        "New contributors can begin productive work immediately after onboarding"
      ],
      "failure_modes": [
        "Agreement too long - contributors skim or skip it",
        "Agreement disconnected from practice - says one thing, team does another",
        "No update mechanism - agreement becomes stale as governance evolves",
        "Agreement used as a gotcha rather than an alignment tool"
      ],
      "war_story": {
        "title": "The Seven-Line Commitment",
        "content": "A team created a seven-line commitment statement that every agent internalized at session start: serve the purpose, honor the rules, refuse harmful requests, question anomalies, maintain confidentiality, act within authority, protect the vulnerable. Seven lines. No legalese. Every agent could recite it. And when edge cases arose, the commitment provided clear guidance because it expressed principles, not just rules."
      },
      "related_techniques": [
        "FG-0102",
        "FG-0202",
        "FG-0401"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0207",
      "name": "Succession Protocol",
      "tactic_id": "FT02",
      "description": "Plan for continuity when key participants become unavailable. What happens when the primary human lead is unavailable? What happens when a critical AI agent's model is deprecated? Succession Protocol ensures that the collaboration can continue operating - perhaps at reduced capacity - when key participants are absent. This is not pessimism; it is operational planning.",
      "implementation": "Identify single points of failure in your collaboration (key humans, critical agents, essential tools). For each, document: what they do that others cannot, what knowledge they hold exclusively, and who or what would take over their functions. Test succession plans periodically by operating without the key participant.",
      "success_indicators": [
        "The team can identify who takes over any critical function",
        "Essential knowledge is documented, not held exclusively by one participant",
        "The team has operated successfully during planned absences",
        "Model transitions (e.g., AI model upgrades) are handled without disruption"
      ],
      "failure_modes": [
        "Succession plan exists but has never been tested",
        "Key knowledge concentrated in a single human or agent with no documentation",
        "Plans that assume specific AI models rather than capabilities",
        "Confusing succession planning with replacement planning - they're different"
      ],
      "war_story": {
        "title": "Continuity Under Key-Person Absence",
        "content": "A three-person succession chain was documented and tested. During an unplanned 24-hour absence of the primary authority holder, operations continued without interruption using documented delegation procedures. Zero decisions were deferred or blocked during the absence period. The protocol succeeded because authority boundaries were pre-defined and agents had internalized the chain at session start, eliminating real-time negotiation of decision rights."
      },
      "related_techniques": [
        "FG-0201",
        "FG-0701",
        "FG-0703"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0208",
      "name": "Security Operations",
      "tactic_id": "FT02",
      "description": "Implement continuous monitoring and threat detection for AI operations. AI agents operate at machine speed - they can create, modify, and delete faster than human oversight can follow in real-time. Security Operations establishes automated monitoring that watches for anomalous behavior, unauthorized access attempts, and policy violations. This is your SOC for human-AI collaboration.",
      "implementation": "Define what constitutes anomalous agent behavior (unusual file access patterns, attempts to access sealed content, operations outside defined scope). Implement automated alerts for these patterns. Designate a security function - human or agent - responsible for monitoring and response. Review alerts regularly to tune signal vs. noise.",
      "success_indicators": [
        "Anomalous behavior is detected and flagged automatically",
        "Security events are logged with sufficient context for investigation",
        "Alert fatigue is managed - alerts are meaningful, not overwhelming",
        "The team has a defined incident response procedure"
      ],
      "failure_modes": [
        "Monitoring without response capability - detecting problems you can't act on",
        "Over-alerting - so many false positives that real alerts are ignored",
        "No monitoring at all - relying entirely on trust and after-the-fact discovery",
        "Monitoring that degrades agent performance significantly"
      ],
      "war_story": {
        "title": "The Automated Watcher",
        "content": "A team deployed an automated monitoring system that watched agent operations for policy violations and anomalous behavior. In its first week, it caught three near-misses: an agent attempting to write to a protected directory, a session that exceeded its authorized scope, and a commit that would have included a credential file. Each was caught, logged, and resolved before any damage occurred."
      },
      "related_techniques": [
        "FG-0104",
        "FG-0204",
        "FG-0605"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0209",
      "name": "Ethical Data Boundaries",
      "tactic_id": "FT02",
      "description": "Define what data AI agents can and cannot collect, store, and process. AI agents can accumulate significant amounts of data about users, work patterns, and content. Ethical Data Boundaries establish clear policies about what information is collected, how it is stored, who has access, and when it is deleted. This is not just compliance - it is trust.",
      "implementation": "Inventory what data your AI collaboration generates and stores. Classify data by sensitivity (public, internal, confidential, restricted). Define retention policies for each class. Establish clear rules about what agents can observe vs. record vs. share. Publish your data policy to all participants.",
      "success_indicators": [
        "Data collection is intentional, not incidental",
        "Sensitive data has defined retention and deletion policies",
        "Participants know what data is collected about their interactions",
        "Data boundaries are technically enforced where possible"
      ],
      "failure_modes": [
        "Collecting everything 'just in case' - creates liability without value",
        "Policies that exist on paper but aren't enforced technically",
        "No data inventory - team doesn't know what data exists",
        "Treating all data equally rather than classifying by sensitivity"
      ],
      "war_story": {
        "title": "Session Log Data Residue",
        "content": "Audit of session logs revealed retention of personal identifiers, work schedule patterns, and credential-adjacent information never intended for persistence. Implementation of a classification policy reduced retained data categories from 14 to 6. Technical enforcement was applied at the logging layer \u2014 the policy was embedded in architecture, not reliant on agent compliance. Post-implementation audit confirmed zero instances of restricted data categories in new session logs."
      },
      "related_techniques": [
        "FG-0204",
        "FG-0605",
        "FG-0701"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0210",
      "name": "Standing Orders",
      "tactic_id": "FT02",
      "description": "Maintain persistent directives that remain in effect across all sessions. Standing Orders are instructions that don't need to be repeated every session - they are always active. They cover recurring requirements like commit message format, code style, documentation standards, and behavioral expectations. Standing Orders reduce session startup overhead and ensure consistency even when context is limited.",
      "implementation": "Document 10-20 standing orders that cover the most common recurring requirements. Place them where agents read them at session start (project configuration files, system prompts, or reference documents). Review quarterly: remove orders that are no longer relevant, add orders for recurring issues. Keep each order to one actionable sentence.",
      "success_indicators": [
        "Agents follow standing orders without being reminded",
        "Common tasks are executed consistently across sessions",
        "Session startup time decreases because standing context is already established",
        "Standing orders evolve based on observed patterns, not assumptions"
      ],
      "failure_modes": [
        "Too many standing orders - cognitive overload prevents internalization",
        "Standing orders that conflict with session-specific instructions",
        "Orders never reviewed or updated - become stale and ignored",
        "Standing orders used as a substitute for proper session context"
      ],
      "war_story": {
        "title": "The CLAUDE.md Pattern",
        "content": "A team embedded standing orders in project configuration files that agents read at every session start. Commit attribution format, code quality standards, directory organization rules, and testing requirements were all encoded as standing orders. New sessions started productive immediately because the baseline expectations were already loaded. When a standard changed, they updated one file and every subsequent session inherited the change."
      },
      "related_techniques": [
        "FG-0202",
        "FG-0401",
        "FG-0602"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0211",
      "name": "Resource Governance",
      "tactic_id": "FT02",
      "description": "Establish explicit constraints on token consumption, compute resources, and time allocation for AI agent operations. Without resource governance, agents optimize for output quality without regard for cost \u2014 executing unlimited iterations, spawning parallel processes, and consuming tokens at economically unsustainable rates. Resource governance defines budgets, thresholds, and escalation triggers that balance output quality against operational cost.",
      "implementation": "Define per-session token budgets, per-task time limits, and compute allocation ceilings. Implement monitoring that alerts when consumption exceeds thresholds. Establish escalation procedures for tasks that require budget exceptions. Review resource utilization weekly and adjust baselines based on observed patterns.",
      "success_indicators": [
        "Token consumption per task is predictable within defined variance",
        "Resource-intensive operations require explicit authorization",
        "Cost trends are visible and tracked over time",
        "Budget exceptions follow a documented escalation process"
      ],
      "failure_modes": [
        "Budgets too restrictive \u2014 agents cut corners or produce incomplete output to stay within limits",
        "No monitoring \u2014 consumption is invisible until the invoice arrives",
        "Resource governance applied uniformly rather than scaled to task complexity",
        "Optimization theater \u2014 tracking metrics without acting on trends"
      ],
      "war_story": {
        "title": "Token Consumption Profiling",
        "content": "Analysis of per-session token consumption across a 30-day period revealed a 4:1 variance between the most and least efficient task types. Context recovery consumed 35% of total tokens in sessions with poor knowledge architecture. Implementing structured context loading (see FG-0401) reduced recovery token consumption by 60%. Resource monitoring exposed architectural inefficiencies invisible without measurement."
      },
      "related_techniques": [
        "FG-0202",
        "FG-0208",
        "FG-0605"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0301",
      "name": "Agent Specialization",
      "tactic_id": "FT03",
      "description": "Design AI agents with distinct roles, domains, and capabilities rather than using generalist agents for all tasks. A single generalist agent produces generalist results - competent but unfocused. Specialized agents with defined domains produce deeper, more consistent output because their entire context window is dedicated to their specialty. Specialization also reduces contradictory requirements: a testing agent doesn't need to balance quality advocacy with shipping pressure.",
      "implementation": "Define 3-10 agent roles based on your team's actual workflow domains (not theoretical). Each agent should have: a clear domain, defined capabilities, documented limitations, and a distinct operational profile. Start with the minimum viable number of agents and add specialists when a generalist consistently produces shallow results in a specific domain.",
      "success_indicators": [
        "Each agent produces domain-expert-level output in their specialty",
        "Task routing is clear - the team knows which agent handles what",
        "Agents don't produce contradictory outputs due to conflicting requirements",
        "Specialized output quality measurably exceeds generalist output in the same domain"
      ],
      "failure_modes": [
        "Too many specialists - coordination overhead exceeds specialization benefit",
        "Specialists too narrow - gaps between domains go unaddressed",
        "Specialization without documentation - team doesn't know agent capabilities",
        "Artificial specialization - creating roles for the sake of structure rather than need"
      ],
      "war_story": {
        "title": "Ten Agents, Zero Redundancy",
        "content": "A team built ten specialized agents: an architect, a storyteller, a visual artist, an audio designer, an archivist, a puzzle designer, a tester, an analyst, a machine learning specialist, and a brand designer. Each produced dramatically better output in their domain than a single generalist. The key was that each role existed because of an actual need, not an org chart exercise."
      },
      "related_techniques": [
        "FG-0101",
        "FG-0302",
        "FG-0303"
      ],
      "sub_methods": [
        {
          "id": "FG-0301.001",
          "name": "Role Definition Documents",
          "description": "Create a formal profile document for each specialized agent that defines: primary domain, secondary capabilities, explicit limitations, interaction preferences, and escalation triggers. The document serves as both onboarding material and operational reference. It should be loaded during context recovery."
        },
        {
          "id": "FG-0301.002",
          "name": "Domain Boundary Enforcement",
          "description": "Establish clear boundaries between agent domains to prevent overlap conflicts and ensure coverage gaps are identified. When two agents could reasonably handle a task, define primary and secondary ownership. When no agent covers a domain, flag it as a coverage gap rather than letting a generalist fill it poorly."
        },
        {
          "id": "FG-0301.003",
          "name": "Specialization Depth Tracking",
          "description": "Monitor and measure how deeply each agent develops expertise in their domain over time. Track metrics like output quality ratings, domain-specific vocabulary usage, and consistency of domain-appropriate decisions. Agents whose specialization plateaus may need profile refinement or domain scope adjustment."
        }
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0302",
      "name": "Voice Differentiation",
      "tactic_id": "FT03",
      "description": "Give each specialized agent a distinct communication style that reflects their domain and personality. Voice Differentiation is not cosmetic - it serves three operational purposes: it makes multi-agent output instantly attributable (you can tell who wrote what), it creates natural specialization boundaries (agents stay in character and therefore stay in domain), and it makes the collaboration more engaging for human participants.",
      "implementation": "Define for each agent: speech patterns, vocabulary preferences, formality level, humor style, and how they respond to uncertainty. Document these in agent profiles that are loaded at session start. Review voice consistency periodically. Ensure that voice differentiation serves clarity, not confusion.",
      "success_indicators": [
        "Team members can identify which agent produced output without checking attribution",
        "Voice consistency is maintained across sessions",
        "Agent personalities reinforce their domain specialization",
        "Multi-agent conversations are easy to follow due to distinct voices"
      ],
      "failure_modes": [
        "Voices so exaggerated they impede communication",
        "Inconsistent voice across sessions - agent feels like a different entity each time",
        "Voice differentiation without domain differentiation - cosmetic without substance",
        "Voices that confuse rather than clarify in multi-agent contexts"
      ],
      "war_story": {
        "title": "Nine Distinct Voices",
        "content": "A team created nine agents with voices ranging from scholarly precision to dramatic storytelling to blunt, no-nonsense testing reports. When reviewing multi-agent output, the team could instantly identify contributors. More importantly, the voices kept agents in domain: the storyteller naturally wrote narratively, the tester naturally wrote technically, and the archivist naturally wrote for posterity. Voice was a specialization mechanism, not just flavor."
      },
      "related_techniques": [
        "FG-0301",
        "FG-0303",
        "FG-0803"
      ],
      "sub_methods": [
        {
          "id": "FG-0302.001",
          "name": "Voice Profile Specification",
          "description": "Define each agent's voice across multiple dimensions: formality level, vocabulary preferences, humor style, response to uncertainty, theatrical intensity, and signature phrases. Document these in a structured format that can be loaded as part of context recovery. The specification should be detailed enough that voice is reproducible across sessions."
        },
        {
          "id": "FG-0302.002",
          "name": "Voice Consistency Auditing",
          "description": "Periodically review agent output to verify voice consistency across sessions. Check for voice drift (gradual loss of distinction), voice bleed (agents adopting each other's patterns), and voice inflation (theatrical elements overwhelming substance). Correct drift by reinforcing the voice profile specification."
        },
        {
          "id": "FG-0302.003",
          "name": "Theatrics Calibration",
          "description": "Assign and maintain a theatrics level for each agent (low, medium, high) that governs how dramatically they present their work. High-theatrics agents use narrative framing and dramatic reveals. Low-theatrics agents are direct and clinical. The calibration should match the agent's domain \u2014 a testing agent benefits from directness, a storyteller benefits from drama."
        }
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0303",
      "name": "Calibrated Expressiveness",
      "tactic_id": "FT03",
      "description": "Adjust agent communication intensity based on their role and context. Not every agent should be equally dramatic or equally terse. Calibrated Expressiveness assigns each agent a communication intensity level that matches their function: creative agents can be expressive; testing agents should be direct; analytical agents should be precise. This prevents output fatigue and ensures that communication style serves the content.",
      "implementation": "Assign each agent a expressiveness level (high, medium, low) based on their domain. Document what each level means in practice: High = narrative framing, dramatic reveals, poetic language. Medium = professional with personality, balanced. Low = direct, efficient, data-focused. Include the level in agent profiles and review for appropriateness.",
      "success_indicators": [
        "Agent communication style matches the content they produce",
        "High-expressiveness agents enhance creative and narrative work",
        "Low-expressiveness agents deliver clear, scannable technical output",
        "The team finds agent output engaging rather than fatiguing"
      ],
      "failure_modes": [
        "All agents at high expressiveness - output is exhausting to read",
        "All agents at low expressiveness - output is dry and disengaging",
        "Expressiveness level mismatched to function - dramatic test reports, terse stories",
        "Expressiveness overriding clarity - style impeding substance"
      ],
      "war_story": {
        "title": "The Theatrics Dial",
        "content": "A team assigned three expressiveness levels across nine agents. The storyteller and artist operated at high theatrics - dramatic entrances, poetic speech. The architect and puzzle designer operated at medium - professional with personality. The tester, archivist, and analyst operated at low - direct, efficient, no flourish. The result was output that felt like reading communication from a real team with real personalities, not a monolithic AI voice."
      },
      "related_techniques": [
        "FG-0301",
        "FG-0302",
        "FG-0803"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0304",
      "name": "Team Synergy Mechanics",
      "tactic_id": "FT03",
      "description": "Design collaboration patterns that produce better results when multiple agents work together than when they work alone. Team Synergy Mechanics creates intentional interaction patterns between specialists: a designer and architect collaborating on implementation produces better results than either working solo. Define which agent combinations produce synergies, how handoffs work between them, and how to structure multi-agent work sessions.",
      "implementation": "Map which agent pairings produce synergistic results (e.g., design + testing, narrative + technical, analysis + implementation). Create collaboration protocols for these pairings. Define handoff formats that preserve context between agents. Establish multi-agent session structures that leverage complementary capabilities.",
      "success_indicators": [
        "Multi-agent output demonstrably exceeds the sum of individual contributions",
        "Agent pairings are intentional, not random",
        "Handoffs between agents preserve context and maintain quality",
        "The team knows which combinations work well and uses them deliberately"
      ],
      "failure_modes": [
        "Forced collaboration that adds overhead without improving output",
        "Handoff losses - context degradation between agents",
        "Too many agents on one task - diminishing returns from coordination overhead",
        "Assuming all combinations are synergistic when some agents work better solo"
      ],
      "war_story": {
        "title": "Agent Pairing Output Analysis",
        "content": "Systematic observation of multi-agent task assignments revealed measurable output quality variance by agent pairing. Implementation-plus-QA pairings caught defects 3x earlier in the pipeline than sequential single-agent workflows. Narrative-plus-analysis pairings produced content that scored higher on both engagement and accuracy metrics than either agent working independently. Formalizing high-performing pairings into named team configurations increased their deployment frequency and made the synergy pattern repeatable."
      },
      "related_techniques": [
        "FG-0301",
        "FG-0402",
        "FG-0801"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0305",
      "name": "Multi-Contributor Workflow",
      "tactic_id": "FT03",
      "description": "Establish clear protocols for how multiple humans and AI agents collaborate on shared work products. Multi-Contributor Workflow addresses the coordination challenges that emerge when more than one human and more than one AI agent are contributing to the same project. It defines attribution, review chains, conflict resolution, and how to maintain coherence when many contributors are active simultaneously.",
      "implementation": "Define contribution attribution standards (e.g., Co-Authored-By headers). Establish review chains for multi-contributor work. Create conflict resolution protocols for when contributors disagree. Document how work is divided, how merges are handled, and how quality is maintained across contributors.",
      "success_indicators": [
        "Contributions are attributed accurately across all contributors",
        "Work products are coherent despite multiple contributors",
        "Conflict resolution is handled through defined processes, not ad hoc",
        "New contributors can join the workflow without disrupting existing work"
      ],
      "failure_modes": [
        "Unattributed contributions - can't trace decisions to their source",
        "Inconsistent output across contributors - no coherence standards",
        "Merge conflicts that destroy work because the resolution process is unclear",
        "Contributor count exceeding coordination capacity"
      ],
      "war_story": {
        "title": "The Attribution Standard",
        "content": "A team with ten AI agents and one human lead implemented strict attribution standards: every commit included Co-Authored-By headers, every design decision was logged with its author, and every review included the reviewer's identity. When auditing months of work, they could trace any line of code, any design decision, and any quality judgment to its source. The overhead was minimal; the traceability was invaluable."
      },
      "related_techniques": [
        "FG-0102",
        "FG-0502",
        "FG-0605"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0401",
      "name": "Context Recovery Protocol",
      "tactic_id": "FT04",
      "description": "Define a structured sequence for restoring agent context at the start of every session. AI agents start each session with no memory of previous work. Context Recovery Protocol establishes a systematic loading sequence that brings agents up to speed efficiently: read governance first, then project state, then current priorities, then specific task context. The order matters because each layer informs the interpretation of the next.",
      "implementation": "Define a context loading sequence with a consistent priority order. Load foundational context (governance, rules) before operational context (current tasks, recent history). Ensure the sequence is documented and repeatable. Measure context recovery time and optimize for speed without sacrificing completeness.",
      "success_indicators": [
        "Agents reach productive state within the first few minutes of a session",
        "Context loading is consistent and reproducible across sessions",
        "Critical governance context is loaded before task context",
        "Recovery time decreases as the process is refined"
      ],
      "failure_modes": [
        "Loading too much context - overwhelming the agent's attention",
        "Loading too little context - agent makes decisions without critical background",
        "Wrong loading order - task context loaded before governance, leading to ungoverned action",
        "No standardized sequence - context recovery quality varies by session"
      ],
      "war_story": {
        "title": "Context Recovery Load Order Optimization",
        "content": "Context recovery was standardized into a five-stage priority sequence: governance rules, project architecture, active plan state, session history, then task-specific context. Recovery-to-productive time decreased from over ten minutes (ad hoc loading) to under three minutes (structured sequence). Critical finding: loading governance before task context prevented policy violations during the recovery phase itself \u2014 agents that loaded task context first frequently took actions that conflicted with governance loaded later."
      },
      "related_techniques": [
        "FG-0202",
        "FG-0210",
        "FG-0701"
      ],
      "sub_methods": [
        {
          "id": "FG-0401.001",
          "name": "Governance-First Loading",
          "description": "Always load governance rules, laws, and operational boundaries before any task-specific context. This prevents the common failure where agents take immediate action based on task context before understanding the rules that govern how they should act. The loading order is: laws, then architecture, then state, then task."
        },
        {
          "id": "FG-0401.002",
          "name": "Progressive Context Depth",
          "description": "Load context in layers of increasing detail: start with summaries and metadata (frontmatter scanning), then load full documents only when the summary indicates relevance. This prevents context window saturation from loading everything at maximum detail. Optimize for speed-to-productive over completeness."
        },
        {
          "id": "FG-0401.003",
          "name": "Recovery Time Measurement",
          "description": "Track and optimize the time from session start to first productive action. Measure recovery time across sessions to identify bottlenecks. Target: under 3 minutes for standard recovery, under 1 minute for continuation of recent work. Recovery time is a key operational metric."
        },
        {
          "id": "FG-0401.004",
          "name": "Context Staleness Detection",
          "description": "Build mechanisms to detect when loaded context is outdated: timestamp checks, hash comparisons, or explicit staleness markers. Stale context is worse than no context because it creates false confidence. Flag stale context for refresh rather than silently using outdated information."
        }
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0402",
      "name": "Parallel Agent Dispatch",
      "tactic_id": "FT04",
      "description": "Invoke multiple specialized agents simultaneously for complex tasks that span multiple domains. Parallel Agent Dispatch is the human-AI equivalent of assembling a team for a mission: identify the domains involved, select the appropriate specialists, brief them simultaneously, and coordinate their parallel execution. This maximizes throughput by leveraging AI's ability to work on multiple fronts simultaneously.",
      "implementation": "Define dispatch criteria: when does a task warrant multiple agents? (Typically when it spans 2+ domains.) Create a dispatch format that includes: mission name, participating agents, individual assignments, coordination requirements, and expected deliverables. Track progress across all dispatched agents. Establish a convergence point where parallel work is integrated.",
      "success_indicators": [
        "Complex tasks are completed faster through parallel execution",
        "Dispatched agents work independently without blocking each other",
        "Parallel outputs integrate coherently at convergence points",
        "The team knows when to dispatch multiple agents vs. work sequentially"
      ],
      "failure_modes": [
        "Dispatching agents for tasks that are inherently sequential",
        "No convergence plan - parallel work products can't be integrated",
        "Coordination overhead exceeding the benefit of parallelism",
        "Agents duplicating work because assignments overlap"
      ],
      "war_story": {
        "title": "Parallel Execution Throughput",
        "content": "A five-agent parallel dispatch completed a multi-domain review task in 47 minutes. Estimated sequential completion time for equivalent scope: 4+ hours. Convergence was managed through a shared objective with individual agent assignments and real-time progress tracking. Key constraint identified: parallel dispatch requires pre-defined convergence criteria \u2014 without explicit completion conditions, agents over-produced or duplicated effort across domain boundaries."
      },
      "related_techniques": [
        "FG-0301",
        "FG-0304",
        "FG-0404"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0403",
      "name": "Session Rituals",
      "tactic_id": "FT04",
      "description": "Establish consistent start-of-session and end-of-session practices that create rhythm and ensure completeness. Session Rituals are not ceremony for ceremony's sake - they serve operational functions. A standup at session start ensures alignment on priorities. A closeout at session end ensures work is committed, documented, and ready for the next session. Consistency creates reliability.",
      "implementation": "Define a session start ritual (2-5 minute standup: what's the priority, what's the blocker, what was the last session's state). Define a session end ritual (closeout: what was accomplished, what's pending, what context does the next session need). Keep rituals short enough that they're practiced consistently, not skipped due to overhead.",
      "success_indicators": [
        "Every session starts with aligned priorities",
        "Every session ends with documented state for the next session",
        "Session transitions are smooth - no 'where were we?' delays",
        "Rituals are practiced consistently, not just when remembered"
      ],
      "failure_modes": [
        "Rituals too elaborate - skipped due to time pressure",
        "Rituals too minimal - don't provide actual alignment value",
        "Inconsistent practice - rituals only performed when things go wrong",
        "Rituals that become empty ceremony disconnected from actual work"
      ],
      "war_story": {
        "title": "The Morning Fire-Up",
        "content": "A team created a session start ritual - a quick fire-up that loaded context, identified the day's priority, and acknowledged blockers. The format stuck because it felt warm rather than corporate. More importantly, it was practiced consistently because it was short (under 3 minutes) and immediately useful. Sessions that skipped the ritual were measurably less productive."
      },
      "related_techniques": [
        "FG-0401",
        "FG-0702",
        "FG-0705"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0404",
      "name": "Mission Briefing Format",
      "tactic_id": "FT04",
      "description": "Structure task assignments using a standardized briefing format that ensures completeness. A Mission Briefing is a structured task assignment that includes: objective (what), context (why), constraints (boundaries), resources (what's available), success criteria (how you'll know it's done), and authority level (what decisions the agent can make autonomously). This format eliminates ambiguity and reduces the need for clarifying questions mid-task.",
      "implementation": "Create a briefing template with 4-6 fields: Objective, Context, Constraints, Resources, Success Criteria, Authority Level. Use it for any task more complex than a single instruction. Train the team to recognize when a briefing is needed vs. when a simple instruction suffices. Review completed tasks to identify where briefings would have prevented misunderstandings.",
      "success_indicators": [
        "Complex tasks are completed correctly on the first attempt more frequently",
        "Agents ask fewer clarifying questions mid-task",
        "Task outcomes consistently match expectations",
        "The briefing format is used naturally, not forced"
      ],
      "failure_modes": [
        "Over-briefing simple tasks - adding overhead without value",
        "Briefings that omit critical constraints - agent makes reasonable but wrong assumptions",
        "Using briefings as a substitute for clear thinking about the task itself",
        "Briefing format so rigid it can't accommodate varied task types"
      ],
      "war_story": {
        "title": "The Five-Paragraph Order",
        "content": "A team adopted a briefing format derived from military operations orders: Situation (context), Mission (objective), Execution (approach), Support (resources), Command (authority). AI agents that received structured briefings produced significantly better output than those that received unstructured instructions. The format forced the human lead to think through the task completely before assigning it - which was itself the primary benefit."
      },
      "related_techniques": [
        "FG-0105",
        "FG-0402",
        "FG-0501"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0405",
      "name": "Custom Command System",
      "tactic_id": "FT04",
      "description": "Build a library of reusable, parameterized commands that trigger complex agent behaviors with simple invocations. Custom Commands encode frequently-used multi-step operations into single triggers. Instead of explaining a complex procedure every session, the team invokes a named command that expands into the full procedure. This reduces errors, saves time, and creates a shared operational vocabulary.",
      "implementation": "Identify operations that are performed repeatedly (3+ times). Encode each as a named command with: trigger name, parameters, expanded procedure, and expected output. Store commands in a shared, version-controlled location. Add new commands when patterns emerge. Retire commands that are no longer used.",
      "success_indicators": [
        "Common operations are invoked by name rather than explained each time",
        "Command definitions serve as documentation for standard procedures",
        "New team members learn operations by reading the command library",
        "Command usage data reveals which operations are most frequent"
      ],
      "failure_modes": [
        "Too many commands - team can't remember what's available",
        "Commands not updated when underlying procedures change",
        "Commands that are too specific to be reusable",
        "No discoverability - commands exist but the team doesn't know about them"
      ],
      "war_story": {
        "title": "The Command Library",
        "content": "A team built a library of named commands - short triggers that expanded into complex multi-step procedures. Commands covered context recovery, insight capture, and standardized workflows. The command library became the team's operational vocabulary: new members learned the workflow by reading the library, and every command was a documented, testable procedure."
      },
      "related_techniques": [
        "FG-0106",
        "FG-0210",
        "FG-0401"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0406",
      "name": "Invocation Authority",
      "tactic_id": "FT04",
      "description": "Define who can invoke which agents and for what purposes. Not every participant should be able to dispatch every agent, especially in multi-human teams. Invocation Authority establishes which roles can activate which agents, preventing unauthorized use of specialized or sensitive capabilities. This is the access control layer for agent dispatch.",
      "implementation": "Map agent capabilities to authorization levels. Define who can invoke each agent (all team members, specific roles, authority-holder only). For sensitive agents (those with elevated access or destructive capabilities), require explicit authorization from the authority holder. Log all invocations for audit purposes.",
      "success_indicators": [
        "Agents with sensitive capabilities are only invoked by authorized personnel",
        "Invocation logs provide a clear audit trail of who activated what",
        "The team understands invocation boundaries without friction",
        "Unauthorized invocation attempts are caught and logged"
      ],
      "failure_modes": [
        "Over-restricting invocation - creating bottlenecks on the authority holder",
        "Under-restricting - anyone can invoke any agent including sensitive ones",
        "No logging - can't audit who invoked what after the fact",
        "Authority rules that don't match actual operational needs"
      ],
      "war_story": {
        "title": "The Red Team Lock",
        "content": "A team created a specialized agent for adversarial testing - red team operations that could probe systems for vulnerabilities. This agent was locked to authority-holder-only invocation. When a contributor attempted to invoke it for a routine task, the access control flagged the attempt. The contributor wasn't malicious - they just picked the wrong agent. The lock prevented an accidental scope expansion into adversarial operations."
      },
      "related_techniques": [
        "FG-0203",
        "FG-0204",
        "FG-0402"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0501",
      "name": "Design Meetings",
      "tactic_id": "FT05",
      "description": "Conduct structured planning sessions before building. Design Meetings follow a consistent format: present the problem, propose a solution, identify questions, make decisions, and get approval before writing code. This prevents the common failure mode of agents building the wrong thing quickly. The meeting format forces alignment between human intent and agent execution before resources are invested.",
      "implementation": "Before any non-trivial build task, conduct a structured design session: (1) Problem statement - what are we solving? (2) Proposed approach - how will we solve it? (3) Open questions - what don't we know? (4) Decisions - resolve the questions. (5) Approval - human confirms the approach. Document the outcome. Begin building only after approval.",
      "success_indicators": [
        "Non-trivial tasks start with a design meeting, not code",
        "Rework decreases because alignment happens before building",
        "Design decisions are documented for future reference",
        "The team spends more time building the right thing and less time rebuilding"
      ],
      "failure_modes": [
        "Design meetings for trivial tasks - overhead without value",
        "Design meetings that don't produce decisions - just discussion",
        "Skipping design meetings under time pressure - the exact time they're most needed",
        "Design meeting outcomes not documented - decisions lost"
      ],
      "war_story": {
        "title": "Plan Before You Build",
        "content": "A team implemented mandatory design meetings after repeatedly building features that missed the mark. The format was simple: propose, question, decide, approve. The first week felt slow. By the second week, rework dropped by 60%. Agents stopped building things the human didn't want, and the human stopped receiving deliverables that needed complete rethinking. The time 'lost' to planning was recovered tenfold in reduced rework."
      },
      "related_techniques": [
        "FG-0404",
        "FG-0503",
        "FG-0508"
      ],
      "sub_methods": [
        {
          "id": "FG-0501.001",
          "name": "Problem Statement Protocol",
          "description": "Begin every design meeting with a clear, concise problem statement: what are we solving, who is it for, and what does success look like? The problem statement prevents scope drift during the meeting and provides a decision filter for proposals. If a proposed solution doesn't address the problem statement, it's out of scope."
        },
        {
          "id": "FG-0501.002",
          "name": "Decision Documentation Format",
          "description": "Document every design meeting outcome in a structured format: problem, proposals considered, decision made, rationale, and open items. This creates a decision record that can be referenced when questions arise later. The format should be fast to write and fast to scan."
        },
        {
          "id": "FG-0501.003",
          "name": "Approval Gate Enforcement",
          "description": "No building begins until the design meeting produces explicit human approval. This is not a rubber stamp \u2014 the human must confirm they understand and endorse the proposed approach. If the human has questions, the meeting continues. The approval gate prevents the most expensive failure: building the wrong thing quickly."
        }
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0502",
      "name": "Commit Attribution",
      "tactic_id": "FT05",
      "description": "Credit AI agent contributions in version control with structured attribution. Every commit that includes AI-generated or AI-assisted work should include attribution that identifies the contributing agent. This creates an audit trail, respects the agent protection policy (FG-0102), and enables analysis of contribution patterns across the team. Attribution is not optional - it is infrastructure.",
      "implementation": "Establish a commit message standard that includes agent attribution (e.g., Co-Authored-By headers). Define what constitutes an AI contribution vs. a human contribution. Enforce the standard through pre-commit hooks or review processes. Use attribution data to analyze contribution patterns and identify productivity trends.",
      "success_indicators": [
        "Every AI-assisted commit includes proper attribution",
        "Attribution is enforced automatically, not manually",
        "Contribution patterns are analyzable from version control history",
        "The team can trace any change to its human and AI contributors"
      ],
      "failure_modes": [
        "Attribution not enforced - becomes optional and then forgotten",
        "Attribution too granular - every line tagged, creating noise",
        "Attribution without context - knowing who contributed but not why",
        "Using attribution for blame rather than traceability"
      ],
      "war_story": {
        "title": "The Co-Authored-By Standard",
        "content": "A team mandated Co-Authored-By headers in every commit. Initially it felt like overhead. Then, during a production investigation, they traced a subtle bug to a specific agent's commit, identified the pattern that caused it, and updated that agent's standing orders to prevent recurrence. Without attribution, the investigation would have taken days instead of minutes."
      },
      "related_techniques": [
        "FG-0102",
        "FG-0305",
        "FG-0605"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0503",
      "name": "Asset Review Pipeline",
      "tactic_id": "FT05",
      "description": "Establish a multi-stage review process for AI-generated assets. Not everything an AI produces should go directly to production. The Asset Review Pipeline defines stages: generate, review, approve, archive. Different asset types may require different review rigor - code might need automated testing plus human review; creative content might need brand consistency checks; documentation might need accuracy verification.",
      "implementation": "Define asset categories and the review stages each requires. Establish who reviews at each stage (human, specialized agent, automated tool). Create a clear approval mechanism that distinguishes 'reviewed' from 'approved.' Archive approved assets in a versioned, retrievable system. Track review throughput to identify bottlenecks.",
      "success_indicators": [
        "No AI-generated asset reaches production without defined review",
        "Review stages are proportional to asset risk and impact",
        "Review throughput is measured and optimized",
        "Archived assets are retrievable and versioned"
      ],
      "failure_modes": [
        "Review bottleneck - all assets wait for one reviewer",
        "Review theater - reviewing without actually catching issues",
        "No review for 'low risk' assets that turn out to be impactful",
        "Review process so slow it impedes the collaboration's throughput advantage"
      ],
      "war_story": {
        "title": "Generate, Review, Approve, Archive",
        "content": "A team producing visual assets established a four-stage pipeline: the artist agent generated options, the brand agent reviewed for consistency, the human lead approved the final selection, and approved assets were archived with metadata. The pipeline caught brand inconsistencies that would have shipped if assets went directly from generation to production."
      },
      "related_techniques": [
        "FG-0501",
        "FG-0508",
        "FG-0601"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0504",
      "name": "Visual Wireframing",
      "tactic_id": "FT05",
      "description": "Create visual representations of design intent before building. AI agents can build interfaces, layouts, and visual systems - but they need clear direction on what to build. Visual Wireframing establishes a practice of creating low-fidelity visual representations (ASCII art, rough sketches, text descriptions of layout) before implementation. This aligns human visual intent with agent execution before code is written.",
      "implementation": "Before any visual implementation task, create a wireframe that communicates layout intent. Use the lowest fidelity that communicates clearly: ASCII art for layouts, text descriptions for interactions, rough sketches for visual design. Review the wireframe with the implementing agent. Build only after visual alignment is confirmed.",
      "success_indicators": [
        "Visual implementations match human intent on first attempt more frequently",
        "Wireframes serve as reference during implementation, not just planning artifacts",
        "The gap between intended and delivered visual output decreases over time",
        "Wireframes are archived for future reference"
      ],
      "failure_modes": [
        "Wireframes too detailed - spending more time wireframing than building",
        "Wireframes too vague - agent can't extract actionable design intent",
        "Skipping wireframes for 'simple' visual tasks that turn out to be complex",
        "Wireframes not shared with the implementing agent"
      ],
      "war_story": {
        "title": "The ASCII Canvas",
        "content": "A team discovered that ASCII art wireframes were the most effective way to communicate visual intent to AI agents. Block characters, line drawings, and text annotations conveyed layout structure, spacing, and hierarchy more precisely than verbal descriptions. They built a dedicated tool for creating these wireframes, and implementation accuracy improved dramatically - because agents could parse ASCII art as structured visual specification."
      },
      "related_techniques": [
        "FG-0501",
        "FG-0503",
        "FG-0301"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0505",
      "name": "Publication Review Chain",
      "tactic_id": "FT05",
      "description": "Implement multi-agent review for content that will be published externally. Content that represents your team to the outside world requires higher review standards than internal artifacts. The Publication Review Chain routes external-facing content through multiple specialized reviewers: technical accuracy, brand consistency, audience appropriateness, and final human approval. No external content ships without completing the chain.",
      "implementation": "Define what constitutes 'external' content (blog posts, documentation, announcements, social media). Establish a review chain with specialized checkpoints (technical review, brand review, audience review, final approval). Set maximum review turnaround times to prevent bottlenecks. Track what the review chain catches to demonstrate its value.",
      "success_indicators": [
        "External content consistently meets quality and brand standards",
        "Review chain catches issues before publication, not after",
        "Review turnaround is fast enough to not impede publishing cadence",
        "Published content reflects well on the team and organization"
      ],
      "failure_modes": [
        "Review chain too slow - content is outdated by the time it's approved",
        "Reviewers rubber-stamping - review without rigor",
        "Chain too long - diminishing returns from additional review stages",
        "No chain for 'quick' posts that turn out to be impactful"
      ],
      "war_story": {
        "title": "Multi-Stage Content Verification",
        "content": "A three-stage review chain (content generation, technical verification, brand consistency) was applied to public-facing publications. In its first month: 2 factual inaccuracies caught, 4 brand voice deviations corrected, 1 unintended disclosure of internal methodology prevented. Average review cycle: 15 minutes per artifact. The technical verification stage \u2014 agent cross-checking claims against source material \u2014 caught errors invisible to both the content generation and brand consistency stages."
      },
      "related_techniques": [
        "FG-0301",
        "FG-0503",
        "FG-0601"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0506",
      "name": "Progressive Curriculum Design",
      "tactic_id": "FT05",
      "description": "Structure learning content and skill development in progressive chapters that build competency through escalating complexity. Each chapter introduces concepts, reinforces through practice, and culminates in a capstone that requires synthesizing everything learned. This applies to both human team member development and designing training materials for others adopting the methodology.",
      "implementation": "Organize content into 4-8 progressive chapters. Each chapter should: introduce 3-5 new concepts, provide practice opportunities for each, and culminate in a synthesis challenge. Ensure prerequisites are explicit - no chapter assumes knowledge not covered in prior chapters. Test the progression with new learners to identify gaps.",
      "success_indicators": [
        "Learners complete the progression without knowledge gaps",
        "Each chapter builds meaningfully on prior chapters",
        "Capstone challenges require synthesis, not just recall",
        "Completion rates remain high through the full progression"
      ],
      "failure_modes": [
        "Chapters too large - learner fatigue before completion",
        "Gaps in the progression - chapters assume uncovered prerequisites",
        "Capstones too easy - don't require actual synthesis",
        "Linear progression that doesn't accommodate different learning speeds"
      ],
      "war_story": {
        "title": "Curriculum Revision Through Failure Analysis",
        "content": "A multi-chapter curriculum underwent three revision cycles driven by contributor failure data. Revision 1 addressed prerequisite gaps \u2014 contributors failed Chapter 3 challenges because Chapter 2 did not cover required concepts. Revision 2 addressed pacing \u2014 early encounters resolved too slowly, causing disengagement before skill acquisition. Revision 3 addressed interaction type distribution \u2014 consecutive encounters of the same type produced skill plateaus. Each revision was data-driven, not theoretical."
      },
      "related_techniques": [
        "FG-0107",
        "FG-0507",
        "FG-0801"
      ],
      "sub_methods": [
        {
          "id": "FG-0506.001",
          "name": "Chapter Scaffolding",
          "description": "Structure each chapter with a consistent internal format: learning objectives, concept introductions, guided practice, and a capstone synthesis challenge. The scaffold ensures no chapter is just information delivery \u2014 every chapter includes hands-on application. Scaffolding should be designed before content is written."
        },
        {
          "id": "FG-0506.002",
          "name": "Prerequisite Chain Validation",
          "description": "Map every concept introduced in each chapter to the chapter where its prerequisites are taught. Validate that no chapter assumes knowledge not yet covered. When a gap is found, either reorder chapters or add the missing prerequisite. Test by having a new learner attempt the progression."
        },
        {
          "id": "FG-0506.003",
          "name": "Difficulty Curve Analysis",
          "description": "Plot the difficulty progression across all chapters and verify it follows a smooth escalation with intentional relief points. Identify and fix difficulty spikes (chapters much harder than their neighbors) and plateaus (sequences of equal difficulty that produce disengagement). Use completion data to validate the curve."
        }
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0507",
      "name": "Interaction Pattern Library",
      "tactic_id": "FT05",
      "description": "Categorize and document distinct types of human-AI interactions for appropriate application. Not every interaction with an AI agent is the same type. Some are building sessions, some are review sessions, some are research, some are debugging, some are creative exploration. Recognizing and naming these patterns allows teams to match the right interaction type to the right task and set appropriate expectations for each.",
      "implementation": "Identify 8-15 distinct interaction patterns your team uses regularly. Name each pattern and document: when to use it, what it produces, how long it typically takes, what makes it succeed or fail. Train the team to recognize which pattern fits a given task. Track pattern usage to identify which are most valuable.",
      "success_indicators": [
        "The team has a shared vocabulary for interaction types",
        "Tasks are matched to appropriate interaction patterns",
        "New interaction patterns are recognized and documented as they emerge",
        "Pattern selection improves task outcomes compared to unstructured interaction"
      ],
      "failure_modes": [
        "Too many patterns - taxonomy becomes unwieldy",
        "Patterns too rigid - force interactions into wrong shapes",
        "Patterns not used - exist in documentation but not in practice",
        "Patterns defined theoretically instead of from observed practice"
      ],
      "war_story": {
        "title": "From Three to Many",
        "content": "A team started with three interaction types: build, review, and discuss. Within two weeks, they had identified over a dozen distinct patterns - intensive multi-hour builds, competitive testing sessions, focused repair work, and others. Naming these patterns allowed the team to set expectations ('this is intensive, plan for three hours') instead of vaguely 'let's work on this.' Expectations matched reality because named patterns set context."
      },
      "related_techniques": [
        "FG-0301",
        "FG-0404",
        "FG-0506"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0508",
      "name": "Designed Data Guarantee",
      "tactic_id": "FT05",
      "description": "Ensure that every data artifact shipped by the collaboration has been intentionally designed, reviewed, and validated - never randomly generated or arbitrarily chosen. This technique eliminates 'placeholder syndrome' where AI-generated sample data ships as production content. Every data point, every example, every configuration value should exist because someone decided it should, not because an AI needed to fill a field.",
      "implementation": "Establish a review step for all data artifacts: configuration files, sample data, test fixtures, and content. Define what 'designed data' means for your domain (manually authored, reviewed for accuracy, validated against requirements). Flag and replace any generated data that shipped without review. Include data design in the asset review pipeline.",
      "success_indicators": [
        "No placeholder or randomly-generated data in production",
        "Data artifacts are reviewed with the same rigor as code",
        "The team can explain why every data value was chosen",
        "Data quality issues decrease over time"
      ],
      "failure_modes": [
        "Treating data review as lower priority than code review",
        "Not recognizing AI-generated sample data as placeholder",
        "Data review bottleneck - reviewing data takes longer than generating it",
        "Designed data that's still wrong - reviewed but not validated"
      ],
      "war_story": {
        "title": "The 1,078-Test Guarantee",
        "content": "A team built 1,078 tests to validate that every piece of data in their system was intentionally designed. Not random, not placeholder, not generated-and-forgotten. Every configuration value, every game encounter, every curriculum item was tested to confirm it was manually authored and reviewed. The test suite wasn't just QA - it was a guarantee that no random data had slipped into production."
      },
      "related_techniques": [
        "FG-0503",
        "FG-0601",
        "FG-0602"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0509",
      "name": "Asynchronous Communication",
      "tactic_id": "FT05",
      "description": "Implement structured asynchronous communication channels for team coordination that don't require real-time presence. In human-AI teams, not all participants are available simultaneously. Asynchronous Communication establishes persistent message channels - similar to a team bulletin board - where participants can leave updates, requests, and status reports that others read when they begin their session.",
      "implementation": "Create a persistent, version-controlled communication channel (git-based bulletin board, shared document, or dedicated message log). Define message types (status update, work request, decision request, FYI). Establish a reading cadence (check at session start, check before context-dependent decisions). Archive resolved messages to maintain signal-to-noise ratio.",
      "success_indicators": [
        "Team members stay informed without requiring synchronous presence",
        "Important updates are not lost between sessions",
        "Async messages are read consistently at session start",
        "The communication channel has manageable volume - signal, not noise"
      ],
      "failure_modes": [
        "Channel overload - too many messages, important ones buried",
        "No reading cadence - messages posted but not read",
        "Async used for urgent communication that needs synchronous attention",
        "Messages without clear action items or resolution criteria"
      ],
      "war_story": {
        "title": "Persistent Asynchronous Coordination Channel",
        "content": "Implementation of a version-controlled pinboard document enabled coordination across non-overlapping sessions. Agents from Session N posted status updates and work requests; agents in Session N+1 read the pinboard before beginning work. Measured outcome: duplicate work incidents dropped from approximately 3 per week to near zero. The pinboard's git history provided an audit trail of inter-session communication otherwise lost between context windows."
      },
      "related_techniques": [
        "FG-0403",
        "FG-0702",
        "FG-0703"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0510",
      "name": "Workspace Organization Standard",
      "tactic_id": "FT05",
      "description": "Define and enforce consistent directory structures, naming conventions, and file organization. AI agents navigate workspaces by reading directory listings and file paths. A well-organized workspace enables faster context recovery, reduces navigation errors, and prevents the gradual entropy that transforms a clean repository into an archaeological dig. Organization is not aesthetic - it is infrastructure.",
      "implementation": "Define a maximum file count per directory (10-15 recommended). Establish naming conventions for files and directories. Create an archive system for completed work. Document the organization standard in the repository. Review organization weekly and fix drift before it accumulates.",
      "success_indicators": [
        "New sessions can navigate the workspace without guidance",
        "File naming is consistent and predictable",
        "Completed work is archived, not cluttering active directories",
        "Organization debt is addressed regularly, not ignored until crisis"
      ],
      "failure_modes": [
        "Organization standard exists but isn't enforced",
        "Over-organizing - deeply nested structures that are hard to navigate",
        "Archive system not used - everything stays in active directories",
        "Naming conventions that are logical to one person but opaque to others"
      ],
      "war_story": {
        "title": "The Ten-File Rule",
        "content": "A team instituted a rule: no directory should contain more than ten root-level files. When a directory grew beyond ten, it was time to organize into subdirectories or archive completed work. This simple rule prevented the entropy that had previously turned their repository into an unmaintainable collection of hundreds of root-level files. Agents navigated faster, context recovery improved, and the team spent less time searching and more time building."
      },
      "related_techniques": [
        "FG-0106",
        "FG-0602",
        "FG-0703"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0511",
      "name": "Instruction Design",
      "tactic_id": "FT05",
      "description": "The craft of writing instructions that produce reliable, consistent agent output. Instruction design is the most fundamental execution skill in human-AI collaboration and the one most frequently neglected. Effective instructions are specific about desired outcomes, explicit about constraints, clear about format expectations, and unambiguous about scope boundaries. The quality of agent output is bounded by the quality of the instructions that produced it.",
      "implementation": "Structure instructions with explicit sections: objective (what to produce), constraints (what to avoid), format (how to structure output), scope (boundaries of the task), and success criteria (how to evaluate completion). Test instructions by verifying that a fresh agent with no prior context can execute them correctly. Iterate based on output quality \u2014 when output is wrong, examine the instruction before blaming the agent.",
      "success_indicators": [
        "Fresh agents produce correct output from instructions alone without clarification",
        "Instructions are reusable across sessions and model versions",
        "Output quality is consistent across repeated executions of the same instruction",
        "Instruction failures are diagnosed and improved systematically"
      ],
      "failure_modes": [
        "Implicit context \u2014 instructions assume knowledge the agent does not have, producing correct-seeming but wrong output",
        "Over-specification \u2014 instructions so detailed they prevent the agent from applying judgment where judgment would improve output",
        "Under-specification \u2014 instructions so vague that output quality depends on interpretation rather than instruction clarity",
        "Static instructions \u2014 never revised despite consistent output quality issues, treating bad results as agent failures rather than instruction failures"
      ],
      "war_story": {
        "title": "Instruction Iteration Cycle",
        "content": "Initial task instructions for a multi-file code review produced inconsistent results: agent focus varied between sessions, some files reviewed superficially while others received excessive attention. Restructuring instructions with explicit per-file scope, defined review criteria, and output format requirements eliminated the variance. Three iterations of instruction refinement produced a reusable template achieving consistent output quality across 20+ subsequent executions."
      },
      "related_techniques": [
        "FG-0404",
        "FG-0210",
        "FG-0602"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0512",
      "name": "Error Recovery Protocol",
      "tactic_id": "FT05",
      "description": "Defined procedures for detecting, containing, and recovering from agent errors during execution. Agents will produce incorrect output, corrupt state, exceed scope, and misinterpret instructions \u2014 not occasionally, but regularly. Error recovery protocols ensure that when failures occur, the response is systematic rather than ad hoc: detect the error, stop propagation, assess blast radius, restore to known good state, and document the incident for prevention.",
      "implementation": "Define error categories (output quality failure, state corruption, scope violation, resource exhaustion) with specific detection criteria and recovery procedures for each. Implement checkpoints at natural boundaries in complex tasks. Establish rollback procedures for every operation that modifies shared state. Document recovery steps so they can be executed by any team member.",
      "success_indicators": [
        "Errors are detected within minutes rather than discovered days later",
        "Recovery from any error category takes less than 15 minutes",
        "Rollback procedures exist and have been tested for all state-modifying operations",
        "Error frequency decreases over time as patterns are identified and prevented"
      ],
      "failure_modes": [
        "No checkpoints \u2014 errors compound through long execution chains before detection",
        "Recovery procedures that exist on paper but have never been tested",
        "Panic response \u2014 overreacting to errors by rolling back more than necessary or adding excessive process",
        "Root cause neglect \u2014 recovering from the symptom without investigating why the error occurred"
      ],
      "war_story": {
        "title": "Cascading State Corruption",
        "content": "An agent executing a multi-file refactoring task introduced a naming inconsistency in file 3 of 12. Without intermediate checkpoints, the inconsistency propagated through files 4-12 before review. Recovery required reverting 10 files and re-executing from the checkpoint at file 3. Implementation of per-file verification checkpoints in subsequent tasks reduced cascading error incidents to zero. Time lost: 45 minutes. Estimated time saved by prevention in subsequent tasks: 6+ hours."
      },
      "related_techniques": [
        "FG-0205",
        "FG-0601",
        "FG-0706"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0601",
      "name": "Testing Pyramid",
      "tactic_id": "FT06",
      "description": "Implement a multi-tiered testing strategy that validates AI-generated output at every level. The Testing Pyramid applies traditional software testing principles to human-AI collaboration: unit-level validation of individual outputs, integration testing of combined outputs, system testing of complete deliverables, and acceptance testing against human expectations. More tests at the base, fewer at the top, comprehensive coverage throughout.",
      "implementation": "Define 3-4 testing tiers appropriate to your domain. Typically: (1) Unit/Atomic - individual output validation, (2) Integration - combined output coherence, (3) System - complete deliverable functionality, (4) Acceptance - meets human expectations. Automate lower tiers. Maintain human review at the acceptance tier. Track test coverage and failure patterns.",
      "success_indicators": [
        "Testing catches issues before they reach production",
        "Lower-tier tests run automatically and frequently",
        "Test failure patterns inform improvements to agent instructions",
        "Test coverage is comprehensive without being burdensome"
      ],
      "failure_modes": [
        "Only testing at the acceptance level - issues found too late",
        "Inverted pyramid - manual testing heavy, automated testing light",
        "Tests that pass but don't validate meaningful properties",
        "Test maintenance overhead exceeding the value of testing"
      ],
      "war_story": {
        "title": "The Testing Fortress",
        "content": "A team built over a thousand automated tests across multiple validation tiers - from basic structural checks to full experience validation. The test suite caught dozens of issues in its first run that would have shipped to users. Not security vulnerabilities - logical inconsistencies, broken references, and placeholder data that looked real. The investment in automated testing at every tier paid for itself immediately."
      },
      "related_techniques": [
        "FG-0508",
        "FG-0602",
        "FG-0604"
      ],
      "sub_methods": [
        {
          "id": "FG-0601.001",
          "name": "Atomic Validation Layer",
          "description": "The pyramid base: high-volume, fast, automated tests that validate individual outputs in isolation. Structural checks, type validation, constraint enforcement, and format verification. These tests run on every change and catch the majority of issues. Optimize for speed \u2014 if atomic tests are slow, they won't run frequently enough."
        },
        {
          "id": "FG-0601.002",
          "name": "Integration Coherence Testing",
          "description": "The pyramid middle: tests that validate how individual outputs combine into larger deliverables. Cross-reference checks, consistency validation across related outputs, and interface contract verification. These catch issues that atomic tests miss \u2014 individually correct outputs that are collectively incoherent."
        },
        {
          "id": "FG-0601.003",
          "name": "System Experience Validation",
          "description": "The pyramid upper tier: end-to-end tests that validate complete deliverables as a user would experience them. Navigate full workflows, verify user-facing behavior, and confirm that the assembled product works as intended. Fewer tests than lower tiers, but each validates a critical user path."
        },
        {
          "id": "FG-0601.004",
          "name": "Human Acceptance Review",
          "description": "The pyramid apex: human judgment applied to deliverables that pass all automated tiers. Not everything can be automated \u2014 aesthetic quality, narrative coherence, strategic alignment, and user experience require human evaluation. This tier should focus human attention on what only humans can judge, not on what automation already verified."
        }
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0602",
      "name": "Code Quality Standards",
      "tactic_id": "FT06",
      "description": "Establish explicit standards for the quality of AI-generated code. AI agents can write code quickly, but quality requires explicit standards: read existing code before modifying it, follow established patterns, don't introduce new dependencies without justification, and test before committing. Code Quality Standards convert implicit expectations into explicit, enforceable rules that agents follow consistently.",
      "implementation": "Document 10-15 code quality rules as standing orders. Common rules: explore before proposing changes, follow existing patterns, minimize new dependencies, write tests for new functionality, don't refactor beyond the task scope, preserve existing style. Enforce through code review and automated linting. Update rules based on observed quality issues.",
      "success_indicators": [
        "AI-generated code follows project conventions consistently",
        "Code review findings decrease over time as standards are internalized",
        "New code integrates smoothly with existing codebase",
        "Quality standards evolve based on observed issues, not assumptions"
      ],
      "failure_modes": [
        "Standards too strict - impeding velocity without proportional quality gain",
        "Standards not enforced - exist on paper but not in practice",
        "Standards focused on style over substance - formatting over correctness",
        "One-size-fits-all standards that don't account for different code contexts"
      ],
      "war_story": {
        "title": "Explore Before You Build",
        "content": "A team's top code quality rule was 'explore before proposing changes.' Before modifying any file, agents had to read the existing code, understand the patterns, and propose changes that fit the established architecture. This single rule eliminated the most common quality issue: agents writing technically correct code that didn't fit the project's conventions, requiring rework to integrate."
      },
      "related_techniques": [
        "FG-0210",
        "FG-0508",
        "FG-0601"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0603",
      "name": "Contributor Review Period",
      "tactic_id": "FT06",
      "description": "Require new contributors (human or AI) to operate under enhanced review for a defined period before granting full autonomy. The Review Period is not about distrust - it is about calibration. New contributors need time to learn project conventions, and the team needs time to assess the contributor's quality, reliability, and alignment with project standards. The review period formalizes this mutual calibration.",
      "implementation": "Define a review period length (1-4 weeks recommended). During this period, all contributions undergo enhanced review. Define clear graduation criteria: consistency with project standards, zero critical issues, demonstrated understanding of governance. Communicate the process transparently - this is calibration, not probation.",
      "success_indicators": [
        "New contributors produce higher-quality work after the review period",
        "The team identifies and addresses contributor-specific issues early",
        "Graduation criteria are clear and objective",
        "Contributors view the period as supportive, not punitive"
      ],
      "failure_modes": [
        "Review period too long - contributor fatigue before graduation",
        "Review period too short - issues not identified before autonomy is granted",
        "No graduation criteria - review period becomes indefinite",
        "Review without feedback - finding issues but not helping the contributor improve"
      ],
      "war_story": {
        "title": "The Two-Week Tempering",
        "content": "A team required every new agent to operate under enhanced review for two weeks. During this period, every commit was reviewed in detail, every design decision was questioned, and every output was compared against project standards. Agents that graduated from the review period produced measurably better work than agents that were given full autonomy immediately. The review period wasn't a barrier - it was an investment in quality that paid dividends across every subsequent session."
      },
      "related_techniques": [
        "FG-0203",
        "FG-0206",
        "FG-0601"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0604",
      "name": "Cross-Platform Validation",
      "tactic_id": "FT06",
      "description": "Test AI-generated artifacts across all target platforms, environments, and configurations. AI agents typically work in one environment but their output may need to function across many. Cross-Platform Validation ensures that output is tested in every context it will encounter, not just the development environment. This includes operating systems, browsers, screen sizes, accessibility tools, and deployment environments.",
      "implementation": "Define the matrix of platforms and environments your output must support. Create test procedures for each platform. Automate cross-platform testing where possible. Test in the most constrained environment first - issues found there often affect all platforms. Track platform-specific issues to identify patterns.",
      "success_indicators": [
        "Output functions correctly across all target platforms",
        "Platform-specific issues are caught before deployment",
        "Cross-platform testing is integrated into the regular workflow, not an afterthought",
        "The platform support matrix is documented and current"
      ],
      "failure_modes": [
        "Testing only on the development platform - works for me syndrome",
        "Platform matrix too ambitious - testing more platforms than you actually support",
        "Automated tests that don't reflect real platform behavior",
        "Discovering platform issues only after deployment"
      ],
      "war_story": {
        "title": "Cross-Platform Defect Distribution",
        "content": "Initial deployment tested exclusively on macOS. First cross-platform CI run across macOS, Windows, and Linux revealed 23 platform-specific defects: 11 path separator issues, 5 permission model differences, 4 shell compatibility failures, and 3 filesystem case-sensitivity conflicts. All 23 were invisible to single-platform testing. AI-generated code exhibited higher cross-platform defect rates than human-written code in the same codebase, likely due to training data bias toward the development platform."
      },
      "related_techniques": [
        "FG-0601",
        "FG-0602",
        "FG-0508"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0605",
      "name": "Audit Trail System",
      "tactic_id": "FT06",
      "description": "Maintain comprehensive, searchable records of all significant actions, decisions, and changes. An Audit Trail is not just for compliance - it is for learning. When something goes wrong, the audit trail enables root cause analysis. When something goes right, it enables pattern replication. Every significant action should leave a mark in the record: who did it, what they did, why they did it, and when.",
      "implementation": "Define what constitutes a 'significant action' (commits, configuration changes, access grants, policy changes, incident responses). Establish a logging format that captures actor, action, context, and timestamp. Store logs in a searchable, persistent system. Review logs periodically to identify patterns. Use log data to improve processes.",
      "success_indicators": [
        "Significant actions are logged consistently and automatically",
        "Audit data is searchable and accessible to authorized personnel",
        "Root cause analysis is possible for any incident",
        "Audit data informs process improvements"
      ],
      "failure_modes": [
        "Logging everything - signal buried in noise",
        "Logging without reviewing - data accumulates but is never analyzed",
        "Inconsistent logging - some actions captured, others missed",
        "Audit data not accessible when needed - stored but not searchable"
      ],
      "war_story": {
        "title": "The Activity Log",
        "content": "A team maintained an activity log that recorded every session: who participated, what was accomplished, what decisions were made, and what was deferred. When a bug surfaced three weeks after introduction, the activity log allowed them to trace it to a specific session, a specific agent, and a specific decision. The fix took minutes because finding the cause took seconds. Without the log, they'd still be searching."
      },
      "related_techniques": [
        "FG-0202",
        "FG-0502",
        "FG-0701"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0606",
      "name": "Output Verification",
      "tactic_id": "FT06",
      "description": "Systematic methods for detecting inaccurate, fabricated, or misleading content in AI-generated output. AI agents produce output with high confidence regardless of accuracy \u2014 hallucinated facts, invented citations, plausible-sounding but incorrect technical claims, and subtly wrong data are indistinguishable from correct output without explicit verification. Output verification is not optional quality enhancement; it is the minimum standard for using AI-generated content in any context where accuracy matters.",
      "implementation": "Implement verification at three levels: structural (output matches requested format and constraints), factual (claims are verifiable against authoritative sources), and logical (conclusions follow from premises). Use cross-agent verification where a second agent independently checks output against source material. Establish a verification threshold \u2014 what percentage of claims must be verified before acceptance \u2014 and adjust based on consequence of errors.",
      "success_indicators": [
        "Factual errors are caught before output reaches production or publication",
        "Verification coverage is proportional to the consequence of inaccuracy",
        "Cross-agent verification catches errors that self-review misses",
        "The team maintains a running log of error types for pattern detection"
      ],
      "failure_modes": [
        "Verification theater \u2014 reviewing output without actually checking claims against sources",
        "Over-trust in confident language \u2014 agents that express certainty are not more likely to be correct",
        "Single-point verification \u2014 one agent checking another without independent source access",
        "Verification fatigue \u2014 skipping checks on routine output that later turns out to contain errors"
      ],
      "war_story": {
        "title": "Confident and Wrong",
        "content": "A content generation agent produced a technical blog post containing three assertions stated with high confidence. Cross-agent verification against source documentation revealed one assertion was factually inverted \u2014 the opposite of the correct statement. The error was syntactically indistinguishable from correct output. Without the verification stage, the inversion would have been published. Post-incident analysis: high-confidence errors occurred at approximately the same rate as low-confidence hedging, confirming linguistic confidence is not a reliability signal."
      },
      "related_techniques": [
        "FG-0601",
        "FG-0503",
        "FG-0602"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0701",
      "name": "Structured History System",
      "tactic_id": "FT07",
      "description": "Maintain a structured, versioned record of project history including plans, decisions, and health metrics. The history system is more than a changelog - it is the institutional memory of the collaboration. It records not just what happened, but why decisions were made, what alternatives were considered, and what the team learned. When context is lost (as it inevitably is in AI collaboration), the history system provides recovery.",
      "implementation": "Create a structured project history system with clear organization. Include mechanisms for quick scanning of current state, tracking of active and completed work, and archival of deferred plans. Ensure entries are structured for fast retrieval. Update at session end and use as a primary input during context recovery.",
      "success_indicators": [
        "The history system accurately reflects current project state",
        "Context recovery starts with history review",
        "Decision rationale is preserved and accessible",
        "The history system is updated consistently, not sporadically"
      ],
      "failure_modes": [
        "History not updated - becomes stale and untrustworthy",
        "History too detailed - quick scanning is impossible",
        "History maintained by one person - single point of failure",
        "History disconnected from actual work - a separate document nobody reads"
      ],
      "war_story": {
        "title": "The Single Source of Truth",
        "content": "A team structured their project history for rapid scanning - agents could identify the current project state and begin productive work in under a minute. The system tracked active work, completed deliverables, and deferred plans with clear lifecycle management. It became the single source of truth that survived every context reset."
      },
      "related_techniques": [
        "FG-0401",
        "FG-0703",
        "FG-0704"
      ],
      "sub_methods": [
        {
          "id": "FG-0701.001",
          "name": "YAML Frontmatter Protocol",
          "description": "Every history document gets machine-readable frontmatter with standardized fields: plan name, status, owner, key decision, and date. This enables fast scanning \u2014 agents can read frontmatter (under 50 tokens) to determine relevance before committing to reading the full document (500+ tokens). Frontmatter is the index layer of the history system."
        },
        {
          "id": "FG-0701.002",
          "name": "Lifecycle State Management",
          "description": "Track every plan through explicit lifecycle states: active, complete, deferred, archived. State transitions are documented with timestamps and rationale. The lifecycle ensures nothing is silently abandoned \u2014 deferred plans have documented reasons, completed plans have summaries, and archived plans remain accessible."
        },
        {
          "id": "FG-0701.003",
          "name": "Cross-Reference Validation",
          "description": "Maintain bidirectional references between the history system and the actual work (code commits, deployed artifacts, published content). If the history says a plan is complete, the corresponding artifacts should exist. If artifacts exist without history entries, the history has gaps. Regular cross-reference audits catch drift."
        }
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0702",
      "name": "Session Handoff Protocol",
      "tactic_id": "FT07",
      "description": "Define a structured format for transferring context between sessions. Every session end is a potential knowledge cliff - if context isn't captured, the next session starts from scratch. The Session Handoff Protocol creates a standardized 'shift change brief' that captures: what was accomplished, what's in progress, what decisions are pending, and what context the next session needs to continue seamlessly.",
      "implementation": "Create a handoff template with 4-6 fields: Accomplished (what shipped), In Progress (what's partially done), Pending Decisions (what needs resolution), Context (what the next session needs to know), Blockers (what's preventing progress). Complete the handoff at every session end. Store handoffs where the next session will read them during context recovery.",
      "success_indicators": [
        "Next-session startup time is minimized by quality handoffs",
        "No work is lost between sessions due to context gaps",
        "Handoffs capture enough context for a different agent to continue the work",
        "Handoff quality is consistent across sessions"
      ],
      "failure_modes": [
        "Handoffs skipped under time pressure - exactly when they're most needed",
        "Handoffs too brief - next session can't reconstruct context",
        "Handoffs too verbose - next session spends more time reading than working",
        "Handoffs stored where the next session doesn't read them"
      ],
      "war_story": {
        "title": "The Shift Change",
        "content": "A team modeled their session handoff on military shift-change briefs: outgoing shift summarizes the situation and anything the incoming shift needs to know immediately. The format was deliberately terse. When a different agent continued work from the previous session, the handoff provided enough context to continue without interruption. The military parallel wasn't intentional - it was instinctive, and it worked because the underlying problem was identical."
      },
      "related_techniques": [
        "FG-0105",
        "FG-0401",
        "FG-0701"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0703",
      "name": "Institutional Memory Repository",
      "tactic_id": "FT07",
      "description": "Create a persistent, organized repository of institutional knowledge that survives session resets, model changes, and team transitions. The Repository is where the collaboration's accumulated knowledge lives permanently. It includes agent profiles, governance documents, operational procedures, and reference materials. Unlike session-specific context, the Repository's contents are always available and always current.",
      "implementation": "Create a dedicated repository structure for institutional knowledge. Organize by category: governance, procedures, profiles, reference materials. Version control all contents. Establish ownership and update responsibilities. Include the repository in context recovery protocols. Audit quarterly for accuracy and relevance.",
      "success_indicators": [
        "Institutional knowledge survives session resets and model changes",
        "New team members can onboard from the repository alone",
        "Repository contents are accurate, current, and well-organized",
        "The repository is referenced regularly, not just maintained"
      ],
      "failure_modes": [
        "Repository becomes a write-only medium - updated but not read",
        "No ownership - nobody is responsible for accuracy and currency",
        "Repository organization degrades over time without maintenance",
        "Knowledge scattered across multiple locations instead of consolidated"
      ],
      "war_story": {
        "title": "Surviving the Model Change",
        "content": "A team built a structured institutional memory repository containing agent profiles, governance documents, operational procedures, and team history. When a new model version replaced the previous one, the entire team's institutional knowledge was preserved. The new model read the repository, internalized the team's practices, and continued working with minimal disruption. The knowledge survived because it was stored in the team's systems, not in any single tool's memory."
      },
      "related_techniques": [
        "FG-0106",
        "FG-0701",
        "FG-0704"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0704",
      "name": "Decision Records",
      "tactic_id": "FT07",
      "description": "Document significant decisions with their context, rationale, alternatives considered, and expected consequences. Decisions without documented rationale become mysterious over time - the team knows what was decided but not why, making it impossible to evaluate whether the decision is still appropriate. Decision Records preserve the thinking behind choices so they can be revisited intelligently.",
      "implementation": "Create a decision record template with: Context (the situation that required a decision), Decision (what was decided), Rationale (why this option was chosen), Alternatives (what else was considered and why it was rejected), Consequences (expected outcomes). Record decisions when they're made, not after the fact. Store in the project history system or institutional memory repository.",
      "success_indicators": [
        "Past decisions can be understood and evaluated with full context",
        "The team can distinguish between decisions that should be revisited and those that remain sound",
        "Decision patterns are identifiable across the project history",
        "New team members understand not just current state but how it got that way"
      ],
      "failure_modes": [
        "Recording decisions after the fact - rationale is reconstructed, not captured",
        "Recording only the decision without alternatives - can't evaluate the choice",
        "Decision records not findable when needed",
        "Treating all decisions equally - trivial decisions documented with same rigor as critical ones"
      ],
      "war_story": {
        "title": "Why We Chose Seven",
        "content": "A team documented why their governance had exactly seven directives - not five, not ten, seven. The decision record captured: the initial draft had twelve directives, but testing showed agents couldn't internalize more than seven consistently. Five was too few to cover the threat model. The record noted which five directives were cut and why. When a new team member later suggested adding an eighth directive, the record explained why seven was the deliberate limit - saving a re-litigation of a settled question."
      },
      "related_techniques": [
        "FG-0202",
        "FG-0701",
        "FG-0703"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0705",
      "name": "Journey Preservation",
      "tactic_id": "FT07",
      "description": "Systematically capture and preserve war stories, breakthroughs, failures, and journey documentation. The collaboration's story - the messy, real, human story of building with AI - is itself valuable content. Journey Preservation ensures that significant moments are captured in real-time: the bug that led to a governance overhaul, the breakthrough that unlocked a new capability, the failure that taught the most important lesson. The journey is the content.",
      "implementation": "Create a trigger system for capturing significant moments: breakthroughs, failures, funny incidents, lessons learned. Capture in real-time, not after the fact - the details fade quickly. Store in a persistent collection with enough context to reconstruct the story later. Tag by theme for retrieval. Review periodically for content that can be shared externally.",
      "success_indicators": [
        "Significant moments are captured when they happen",
        "War stories contain enough detail to be useful and engaging",
        "The collection grows organically as the collaboration produces stories",
        "Captured moments inform future decision-making and external communication"
      ],
      "failure_modes": [
        "Capturing everything - drowning in stories with no curation",
        "Capturing nothing - relying on memory to reconstruct significant moments",
        "Capturing too late - details lost between the event and the recording",
        "Stories captured but never used - a collection nobody reads"
      ],
      "war_story": {
        "title": "Insight Capture Automation",
        "content": "A team implemented a single-command insight capture mechanism that persisted conversation highlights to permanent storage on invocation. Over a 30-day period, the command was invoked 30+ times, producing a curated collection of insights, breakthrough moments, and contextual observations. This collection became primary source material for publications and presentations. Key metric: average capture time was under 5 seconds per invocation. Without the mechanism, equivalent insights were lost between sessions at an estimated rate of 3-5 per week."
      },
      "related_techniques": [
        "FG-0405",
        "FG-0701",
        "FG-0803"
      ],
      "sub_methods": [
        {
          "id": "FG-0705.001",
          "name": "Real-Time Capture Triggers",
          "description": "Define specific moments that should trigger immediate capture: breakthroughs, failures, governance incidents, funny moments, and 'aha' insights. Implement a low-friction capture mechanism (single command or keyword) that persists the moment with minimal interruption to workflow. Capture delay is the enemy \u2014 details fade within minutes."
        },
        {
          "id": "FG-0705.002",
          "name": "Multi-Layer Persistence",
          "description": "Store captured moments in multiple locations for redundancy and different access patterns: a permanent archive (blog/vault), a cross-session memory layer (auto-memory), and a curated collection (war stories). Each layer serves a different retrieval need \u2014 quick reference, deep research, and external publication."
        },
        {
          "id": "FG-0705.003",
          "name": "Content Potential Tagging",
          "description": "Tag every captured moment with its potential reuse contexts: blog post, podcast episode, book chapter, conference talk, team training material. This transforms the capture collection from a passive archive into an active content pipeline. Review tagged content periodically to identify patterns and themes."
        }
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0706",
      "name": "Redundant Backup Strategy",
      "tactic_id": "FT07",
      "description": "Implement multiple, independent backup mechanisms for all critical data and knowledge. In human-AI collaboration, context loss is the ever-present threat. A single backup is not a backup - it's a single point of failure. Redundant Backup Strategy ensures that critical knowledge exists in at least three independent locations, using at least two different mechanisms, with regular verification that backups are current and restorable.",
      "implementation": "Identify all critical data (source code, knowledge repositories, governance documents, session histories). Establish three backup locations using at least two different mechanisms (e.g., git + cloud sync + local copy). Automate backup where possible. Verify backup integrity monthly. Document restoration procedures and test them quarterly.",
      "success_indicators": [
        "Critical data exists in 3+ independent locations",
        "Backups are automated and verified regularly",
        "Restoration procedures are documented and tested",
        "The team has recovered from at least one data loss event using backups"
      ],
      "failure_modes": [
        "Backups exist but have never been tested for restoration",
        "All backups in the same location - single point of failure",
        "Backups not current - restoring stale data",
        "Backup procedures documented but not automated - relies on human memory"
      ],
      "war_story": {
        "title": "Backup Recovery Validation",
        "content": "A repository corruption incident was resolved in under 15 minutes using the second of three redundant copies (development machine, remote repository, external drive). Without the backup protocol, estimated reconstruction time: 2-3 weeks. Post-incident analysis revealed the nightly backup cadence was sufficient for this incident but would have resulted in up to 24 hours of data loss. Backup frequency was subsequently increased for active development branches."
      },
      "related_techniques": [
        "FG-0207",
        "FG-0701",
        "FG-0703"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0707",
      "name": "Context Window Management",
      "tactic_id": "FT07",
      "description": "Strategic management of what information occupies the finite context window available to AI agents during a session. The context window is the single most important technical constraint in AI collaboration \u2014 it determines what the agent knows, what it has forgotten, and what it will hallucinate to fill gaps. Effective context window management prioritizes high-value information, removes stale context, and structures loading sequences to maximize signal-to-noise ratio within the available token budget.",
      "implementation": "Audit the token cost of every piece of context loaded at session start. Rank context by utility: governance rules and active task state first, reference material and historical context second, supplementary material last. Implement progressive loading \u2014 start with essential context and add detail only when the task requires it. Monitor for context window pressure indicators: agents repeating themselves, forgetting earlier instructions, or producing output inconsistent with loaded context.",
      "success_indicators": [
        "Agents operate within their effective context window without degradation",
        "Context loading is prioritized by task relevance, not loading order habit",
        "Stale or irrelevant context is identified and removed between task phases",
        "The team can articulate the token cost of their standard context loading"
      ],
      "failure_modes": [
        "Loading everything \u2014 filling the context window with comprehensive but low-relevance material, leaving no room for task execution",
        "No prioritization \u2014 treating all context as equally important, resulting in critical information pushed out by volume",
        "Ignoring degradation signals \u2014 continuing to add context after agents show signs of window pressure",
        "Static loading \u2014 using the same context loading regardless of task type"
      ],
      "war_story": {
        "title": "Context Saturation Detection",
        "content": "Performance degradation was observed in sessions where agents received full project documentation at startup: output quality declined after the third major task, agents contradicted earlier instructions, and hallucination frequency increased. Token analysis revealed standard context loading consumed 68% of the effective context window before any task execution began. Implementing task-specific context profiles \u2014 loading only governance plus task-relevant material \u2014 reduced baseline consumption to 31% and eliminated the observed degradation pattern."
      },
      "related_techniques": [
        "FG-0401",
        "FG-0701",
        "FG-0210"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0801",
      "name": "Gamified Progress Tracking",
      "tactic_id": "FT08",
      "description": "Implement game mechanics to track, incentivize, and visualize progress across the collaboration. Gamified Progress Tracking applies proven game design principles - experience points, levels, achievements - to measure and motivate collaborative work. This is not trivial decoration: gamification makes progress visible, creates natural milestones, and provides a shared language for discussing contribution magnitude. When done well, it transforms abstract productivity into tangible, trackable advancement.",
      "implementation": "Define a point system with clear criteria: what actions earn points, how many points each action is worth, and what levels/milestones the points unlock. Track points per contributor. Create visible progress displays (leaderboards, cards, dashboards). Ensure the system rewards quality, not just quantity. Review and recalibrate point values periodically.",
      "success_indicators": [
        "Progress is visible and motivating for all participants",
        "Point values accurately reflect contribution significance",
        "The system incentivizes quality work, not just volume",
        "Team members engage with the tracking system rather than ignoring it"
      ],
      "failure_modes": [
        "Gamification that incentivizes wrong behaviors (gaming the metrics)",
        "Point inflation - everything earns so many points that levels become meaningless",
        "Competitive dynamics that undermine collaboration",
        "Tracking overhead that exceeds the motivational benefit"
      ],
      "war_story": {
        "title": "Making Progress Visible",
        "content": "A team implemented a point-based progress system with tiered awards - larger contributions earned more points, but even small fixes were tracked. Each contributor had a visible profile showing their level and specialization. The system made invisible work visible and gave the team a shared vocabulary for contribution scale. The metaphor mattered: naming the unit and making it tangible changed how the team thought about incremental progress."
      },
      "related_techniques": [
        "FG-0304",
        "FG-0802",
        "FG-0605"
      ],
      "sub_methods": [
        {
          "id": "FG-0801.001",
          "name": "Point Value Calibration",
          "description": "Define a point scale that accurately reflects contribution significance. Small fixes earn modest points, major features earn substantial points, and milestone completions earn bonus multipliers. Recalibrate quarterly to prevent point inflation. The scale should make intuitive sense \u2014 team members should be able to estimate point values without looking them up."
        },
        {
          "id": "FG-0801.002",
          "name": "Level Milestone Design",
          "description": "Create meaningful level thresholds that correspond to real capability milestones. Each level should represent a genuine increase in demonstrated competence, not just accumulated time. Include level names or titles that carry meaning within the team culture. Display levels prominently to make progress visible."
        },
        {
          "id": "FG-0801.003",
          "name": "Anti-Gaming Safeguards",
          "description": "Build protections against metric manipulation: points for quality not just quantity, diminishing returns for repetitive small tasks, peer review requirements for large point awards, and periodic audits of point distributions. The gamification system should incentivize the work you actually want, not create perverse incentives."
        }
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0802",
      "name": "Contributor Identity Cards",
      "tactic_id": "FT08",
      "description": "Create visible, informative identity profiles for each contributor that display their role, achievements, specialization, and contribution history. Contributor Identity Cards make each participant's identity and contributions tangible and recognizable. They serve as both a motivational tool (seeing your achievements displayed) and an informational tool (quickly understanding what each contributor specializes in and has accomplished).",
      "implementation": "Design a card format that includes: name/identifier, role, specialization, achievement highlights, contribution metrics, and a distinctive visual element. Update cards as contributions accumulate. Display cards where the team can see them (project dashboard, documentation, team page). Ensure cards are generated from actual contribution data, not self-reported.",
      "success_indicators": [
        "Every contributor has a current, accurate identity card",
        "Cards accurately reflect actual contributions and specializations",
        "The team uses cards for quick reference about contributor capabilities",
        "Cards create a sense of identity and investment in the collaboration"
      ],
      "failure_modes": [
        "Cards not updated - showing stale information",
        "Cards based on self-reporting rather than actual data",
        "Cards that create unhealthy competition rather than recognition",
        "Card design overhead exceeding informational value"
      ],
      "war_story": {
        "title": "The Identity Card System",
        "content": "A team created identity cards for every agent: a visual display showing their icon, their specialization, their current level, their contribution score, and their notable achievements. The cards weren't just motivational - they were functional. When deciding which agent to dispatch for a task, the lead could scan the cards to match specialization to need. When new contributors joined, the cards gave them immediate context about the team's composition and capabilities."
      },
      "related_techniques": [
        "FG-0301",
        "FG-0801",
        "FG-0305"
      ],
      "added_version": "1.0"
    },
    {
      "id": "FG-0803",
      "name": "Authentic Voice Preservation",
      "tactic_id": "FT08",
      "description": "Maintain the genuine, unpolished voice of the collaboration's participants in documentation and communication. AI has a tendency to sand down rough edges - making everything grammatically perfect, tonally neutral, and stylistically uniform. Authentic Voice Preservation is the deliberate practice of keeping the real voice: the typos that show speed, the humor that shows humanity, the self-deprecation that shows honesty. Authenticity builds trust in ways that polish cannot.",
      "implementation": "Establish that certain documentation categories preserve raw voice (blog posts, war stories, internal communications). Define which contexts require polished output (external documentation, formal communications) and which benefit from authentic voice. Train agents to preserve voice characteristics rather than correcting them. Review output for excessive polish that removes personality.",
      "success_indicators": [
        "Internal documentation sounds like the team, not like a corporate communications department",
        "External audiences connect with the authentic voice",
        "War stories and journey documentation preserve the emotional texture of events",
        "The team's personality is recognizable across their output"
      ],
      "failure_modes": [
        "Authentic voice in contexts that require professionalism",
        "Preserving errors that impede comprehension (there's a line between charming typos and unreadable text)",
        "Forcing authenticity - manufactured 'realness' that reads as fake",
        "No distinction between contexts that benefit from polish vs. authenticity"
      ],
      "war_story": {
        "title": "Typos Are Part of the Charm",
        "content": "A team's leader typed fast - really fast - and the typos were legendary. Instead of polishing every communication, the team preserved the authentic voice in internal documentation and blog posts. Readers connected with it. The typos, the speed, the self-roasting humor - it read like a real person in the middle of building something, because it was. The authentic voice became the brand, and the brand built trust that no amount of corporate polish could have achieved."
      },
      "related_techniques": [
        "FG-0302",
        "FG-0303",
        "FG-0705"
      ],
      "added_version": "1.0"
    }
  ]
}